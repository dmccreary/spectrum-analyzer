{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spectrum Analyzer","text":"<p>Welcome to the website on using the Raspberry Pi Pico 2 as a spectrum analyzer.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Spectrum Analyzer Course Description","text":"<p>This site supports a fun hands-on 14-week course on understanding data transformation using low-cost components.  Although we target high-school students, we have content that is designed to be used by both junior-high students as well as college-level students studying signal processing and digital signal processing.</p> <p>The underlying goals of this course is to teach principals of computational thinking, problem solving, data literacy and visualization of data transformations.  These are universal skills that can be applied in many other areas of STEM education.</p> <p>Although many of the exercises in this class focus on software, there are no requirements that you have a strong software background to learn things from this course.  Students that have a background in Python or JavaScript will have more opportunities to customize the labs, but there is plenty of content for non-programmers.</p>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<p>This course makes an assumption that all students will have access to the internet and know how to use a keyboard, mouse and a web browser.  No coding skills are required.</p>"},{"location":"course-description/#concepts","title":"Concepts","text":"<ul> <li>What is a signal</li> <li>What is a tone</li> <li>What is pitch or frequency of a signal</li> <li>How sounds are composed of multiple frequencies of sounds</li> <li>What are base and treble controls</li> <li>How base and treble controls change the frequency response of music</li> <li>What instruments like a base create low notes</li> <li>What instruments like cymbals create high notes</li> <li>What is a sound frequency spectrum</li> <li>How do we visualize sound in real time</li> <li>Filters: ways to separate sound frequencies</li> <li>Crossovers: sending low notes to a subwoofer</li> <li>Digital Signal Processing</li> <li>Parts of a waveform: frequency, amplitude and phase</li> <li>Waveform types: sine, square, triangle and sawtooth</li> <li>Combining waves of different frequencies</li> <li>Decomposing a complex wave into components</li> </ul>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#term","title":"Term","text":"<p>This is the definition of the term.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":""},{"location":"references/#fft-references","title":"FFT References","text":"<ol> <li>Peter Hinch's GitHub with FFT in ARM assembler for the Pico</li> <li>CMSIS Sandbox JP Trainor GitHub Repo</li> <li>Library with FFT functions for the Pico Googool GitHub Repo</li> <li>ARM v7-M Architecture Reference Manual</li> </ol>"},{"location":"references/#site-build-tool-references","title":"Site Build Tool References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"research/","title":"Research","text":"<p>Prompt</p> <p>I have a Raspberry Pi Pico 2 that has a dual core RP2350 system on a chip.  Each core is a Cortex-M33 with floating point and DSP support.</p> <p>I want to run sound signal processing on these devices and run algorithms like the FFT that leverage the new floating point and the new DSP support.</p> <p>Can you fine any documentation and sample FFT code that has been finely tuned for the Pico 2?</p> <p>I am already aware of Peter Hinch's implementation of FFT in ARM assembly from his GitHub repo.  However, his code is over 10 years old and clearly does not use any of the new features on the Pico 2.</p> <p>Note that Peter's FFT in ARM assembly is here: https://github.com/peterhinch/micropython-fourier/blob/master/dft.py Note that it has not changed since 2015.</p> <p>I am looking for authoritative information and working code that will take advantage of every feature of the Pico 2 including using dual-cores to read in sound and write FFT spectrums to a display.  I would prefer code that also has a test suite and benchmarking tools on synthetic data.</p> <p>Great, I\u2019ll look for documentation and optimized FFT code for the Raspberry Pi Pico 2 (RP2350) that is compatible with MicroPython and designed to leverage its dual-core Cortex-M33 architecture, FPU, and DSP support. I\u2019ll also search for examples that include performance benchmarks or testing tools, ideally with MicroPython calling into assembly or C. I\u2019ll get back to you shortly with what I find.</p>"},{"location":"research/#raspberry-pi-pico-2-fft-documentation-optimized-implementations","title":"Raspberry Pi Pico 2 FFT: Documentation &amp; Optimized Implementations","text":""},{"location":"research/#hardware-dsp-capabilities-of-rp2350-pico-2","title":"Hardware DSP Capabilities of RP2350 (Pico 2)","text":"<p>The Raspberry Pi Pico 2 is built on the RP2350 microcontroller, which features dual Arm Cortex-M33 cores running up to 150\u00a0MHz. In contrast to the original Pico\u2019s M0+ cores, the M33 cores include a single-precision FPU (Floating Point Unit) and Armv8-M DSP instructions (Introducing the RP2350 | Stephen Smith's Blog). This means the Pico 2 can perform floating-point math in hardware and use SIMD-like DSP operations (e.g. single-cycle multiply-accumulate) for faster signal processing. Official datasheets and ARM\u2019s documentation note these enhancements, which significantly improve algorithms like FFT on this platform (Introducing the RP2350 | Stephen Smith's Blog). (The RP2350 also has a dual-core RISC-V cluster, but only one of the two architectures can run at a time.) </p>"},{"location":"research/#authoritative-references","title":"Authoritative references","text":"<ul> <li>RP2350 Datasheet / Tech Docs: The Raspberry Pi Foundation provides documentation for the RP2350, detailing its Cortex-M33 cores, FPU support, and DSP instruction set. These instructions accelerate common DSP operations (fixed-point arithmetic, saturating Q15/Q31 math, etc.), benefiting FFT and filtering routines.  </li> <li>ARM Cortex-M33 Docs: ARM\u2019s official manuals describe the M33\u2019s instruction set and extensions. Notably, the M33\u2019s DSP extension adds optimized arithmetic instructions, and the single-precision FPU executes IEEE754 float operations in hardware (Introducing the RP2350 | Stephen Smith's Blog). Together, these enable much faster FFT computation than on the original Pico (which lacked an FPU or DSP extensions).</li> </ul>"},{"location":"research/#optimized-fft-libraries-for-cortex-m33-rp2350","title":"Optimized FFT Libraries for Cortex-M33 (RP2350)","text":"<p>ARM CMSIS-DSP Library: The CMSIS-DSP library is an ARM-developed collection of highly optimized DSP functions for Cortex-M cores (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). It includes FFT implementations for various data types (32-bit float, 64-bit float, Q15, Q31 fixed-point) that automatically leverage the hardware FPU and DSP instructions when available (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). For example, CMSIS-DSP provides functions like <code>arm_rfft_fast_f32</code> (for real FFT on floats) and <code>arm_cfft_q15</code> (complex FFT on Q15 fixed-point) among many others (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). On the Pico 2\u2019s M33, these routines run very efficiently \u2013 using the FPU for floating-point FFTs or SIMD fixed-point arithmetic for Q15/Q31 FFTs. ARM\u2019s documentation explains that the \u201cFast\u201d real-FFT algorithms exploit symmetry to halve the computation versus naive complex FFTs (CMSIS-DSP: Real FFT Functions) (CMSIS-DSP: Real FFT Functions). </p> <p>Key resources: - CMSIS-DSP Documentation \u2013 The user manual (on ARM\u2019s GitHub pages) details each FFT function and usage (CMSIS-DSP: Real FFT Functions). It covers initialization (e.g. <code>arm_rfft_fast_init_f32</code>), input/output data formats, and optimized algorithms for real data (CMSIS-DSP: Real FFT Functions) (CMSIS-DSP: Real FFT Functions). This is an authoritative reference for how to use these functions and what hardware features they exploit. - CMSIS-DSP Example Code \u2013 There are many examples of using CMSIS-DSP FFT on microcontrollers (for instance, STM32 examples and application notes (STM32 Fast Fourier Transform (CMSIS DSP FFT) - Phil's Lab #111)). These can be adapted to Pico 2. A community project by jptrainor benchmarked CMSIS-DSP on the original Pico, demonstrating how to integrate the library (in C/C++) and measure performance (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). (On an M0+ Pico, float FFTs had to rely on software, but on the M33 Pico 2 the same code will use the hardware FPU for a big speedup.)  </p> <p>KISS FFT (Pico FFT Library): Another option is the lightweight KISS FFT (\u201cKeep It Simple FFT\u201d) library, which is written in portable C. A community member created a Pico-oriented wrapper called pico_fft based on KISS FFT (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.). It simplifies capturing ADC data and computing an FFT on the Pico. While KISS FFT isn\u2019t as optimized as CMSIS-DSP, it\u2019s easier to understand and has a small footprint. The pico_fft project includes documentation and examples of connecting an analog microphone to the Pico and performing an FFT on the samples (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.). All examples (in C using the Pico SDK) have been tested, and they even provide a quick-start guide for hardware setup and code building (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.) (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.). This could be useful if you prefer a self-contained library. Keep in mind that on the RP2350, a KISS FFT (written in C) will still benefit from the M33\u2019s speed and can use the FPU for float math (via the compiler) \u2013 but it won\u2019t explicitly use SIMD DSP instructions unless manually optimized. </p> <p>Performance notes: On the Pico 2, a well-optimized FFT is quite fast. For instance, Peter Hinch\u2019s assembly FFT (discussed below) can compute a 1024-point real FFT in about 7\u00a0ms on a Pico 2 (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). In comparison, the same on a classic Pico M0+ might take tens of milliseconds or more since it lacks the FPU (python - Raspberry Pi Pico(RP2040 or RP2350) ASM PIO microPython for FFT, DSP - Signal Processing Stack Exchange) (python - Raspberry Pi Pico(RP2040 or RP2350) ASM PIO microPython for FFT, DSP - Signal Processing Stack Exchange). CMSIS-DSP\u2019s routines should achieve similar performance to the assembly approach, as they are built to leverage the same hardware features. The CMSIS library also supports smaller FFT sizes efficiently (e.g. 256-point, 512-point), often using mixed-radix algorithms for speed. ARM\u2019s benchmarks (in CMSIS docs) show orders of magnitude speedup vs. naive FFT implementations thanks to these optimizations (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). </p>"},{"location":"research/#dual-core-audio-processing-examples","title":"Dual-Core Audio Processing Examples","text":"<p>One advantage of the RP2350 (Pico 2) is its dual-core CPU, which you can use to parallelize tasks \u2013 ideal for real-time audio capture and FFT display. A great example is Iwatake\u2019s dual-core spectrum analyzer project. It runs on the original Pico (RP2040) but demonstrates the concept well: Core0 handles data acquisition from a microphone via ADC (using DMA) and driving an SPI display, while Core1 performs the FFT calculations on the incoming data stream (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io). This way, the heavy math on Core1 doesn\u2019t stall sampling or screen updates on Core0. The project was written in C++ using the official Pico SDK, and Iwatake has released the full source code on GitHub (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io) (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io). (The code uses ADC + DMA to fill buffers, triggers an IRQ or flag when a buffer is ready, then Core1 processes the buffer with an FFT and sends the result to Core0 for display.) This design achieved a functional real-time spectrum display, although the author noted a occasional bug causing freezes (likely a synchronization issue) (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io). It\u2019s a useful reference for structuring dual-core tasks and using the Pico\u2019s multicore FIFO or interrupts to communicate between cores.</p> <p>Another community example is a project by Van Hunter Adams at Cornell: Realtime Audio FFT to VGA on RP2040. While it ran on a single-core (RP2040) with heavy use of DMA and PIO, it illustrates high-speed ADC capture (10 kHz) with DMA, and performing a 1024-point fixed-point FFT on the fly (FFT) (FFT). On the Pico 2, one could adapt this idea and split the work across cores for even better throughput (e.g., Core0 managing continuous DMA ADC sampling, Core1 doing the FFT and sending results to a VGA or LCD driver). The code and write-up for that project are available on Adams\u2019s site (FFT) (FFT) and could serve as a starting point for handling real-time data streams.</p> <p>For a more hobbyist-friendly approach, there\u2019s an Arduino-based spectrum analyzer for RP2040 by Bodmer that uses one core but cleverly combines DMA and an SPI TFT update. It samples ADC at ~14 kHz, computes an FFT (64\u2013512 points), and draws the spectrum on a 320x240 TFT. The code uses the Earle Philhower Arduino core (which has built-in support for the RP2040) and likely leverages the CMSIS-DSP library under the hood (the Arduino <code>mbed_rp2040</code> core includes CMSIS-DSP) or a simple FFT routine. It achieves about 54 FPS for a 256-point FFT visualization (and up to 81 FPS for 64-point) by using DMA for sampling and fast SPI for drawing (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT) (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT). The source is available on GitHub as \u201cADC_DMA_FFT\u201d with documentation in the README (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT) (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT). While this was tested on RP2040, the same code on an RP2350 could be tweaked to use both cores or simply enjoy the extra performance headroom (e.g. to increase FFT size or sample rate). It\u2019s a good, well-documented reference for implementing an end-to-end audio FFT pipeline on the Pico family.</p> <p>Key takeaways for dual-core design: Use DMA to offload data movement (e.g. ADC to memory), use one core exclusively for I/O (sensor reads, drawing to display) and the other for computation, and use thread-safe queues or interrupts to hand off data between cores. The Pico SDK provides <code>multicore_fifo_push</code>/<code>pop</code> functions and even higher-level primitives to coordinate the two cores. In FreeRTOS or other RTOS environments, you could assign tasks to cores and use mutexes/queues. The examples above show that a dual-core Pico can comfortably handle real-time audio FFT at audio-rate sampling (8\u201344 kHz) and update a display, especially when each core\u2019s workload is optimized.</p>"},{"location":"research/#micropython-and-assembly-integration","title":"MicroPython and Assembly Integration","text":"<p>MicroPython on the Pico 2 lets you write high-level code, but Python by itself is too slow for real-time FFT on microcontrollers. The good news is MicroPython supports calling assembly-optimized routines for performance-critical parts. There are a few ways to do this:</p> <ul> <li> <p>Inline ARM Thumb Assembly: MicroPython has an inline assembler for ARM Cortex-M. By decorating a function with <code>@micropython.asm_thumb</code>, you can write pure assembly instructions that MicroPython will assemble and execute natively (Adding Assembly Language to MicroPython | Stephen Smith's Blog) (Adding Assembly Language to MicroPython | Stephen Smith's Blog). You can pass a few arguments in registers (r0\u2013r3) and use any ARMv7E-M Thumb-2 instructions \u2013 including those for floating point on M33 (MicroPython\u2019s assembler includes opcodes like <code>vadd.f32</code>, etc., which will use the FPU) (Adding Assembly Language to MicroPython | Stephen Smith's Blog). This is ideal for writing a custom FFT inner loop or other DSP routines. The official docs and tutorials (e.g. Damien George\u2019s hints, or Stephen Smith\u2019s blog (Adding Assembly Language to MicroPython | Stephen Smith's Blog)) cover how to structure these functions. Keep in mind you must manage registers and follow the MicroPython calling convention, but you can mix Python and assembly seamlessly (call the asm function from Python with normal arguments). </p> </li> <li> <p>FFI / Native Modules: Another approach is to compile C or assembly code as a native module and import it in MicroPython. MicroPython\u2019s build system allows adding C extensions that become importable as Python modules. For example, one could compile the CMSIS-DSP library (or just the needed FFT function) into the firmware and expose a Python-callable function. This requires customizing the firmware build, but there are community discussions and examples on doing this (for instance, wrapping CMSIS-DSP functions in a MicroPython module) (CMSIS DSP and NN micropython wrappers \u00b7 micropython \u00b7 Discussion #10200 \u00b7 GitHub). There isn\u2019t an off-the-shelf MicroPython \u201cnumpy\u201d with DSP yet (though it\u2019s been contemplated (DSP CMSIS - MicroPython Forum (Archive)) (DSP CMSIS - MicroPython Forum (Archive))), but the ulab module is a notable alternative. ulab is a NumPy-like module for MicroPython (written in C) that provides vectorized operations and an FFT routine (<code>ulab.numpy.fft.fft</code>) in a Pythonic way (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). It\u2019s not specific to the RP2350, but it\u2019s optimized C code, so it runs much faster than Python loops and can handle moderate-sized FFTs. If using MicroPython, you might consider using ulab\u2019s FFT for simplicity \u2013 it won\u2019t explicitly use the M33 DSP instructions, but in C it will still be fast, and you avoid writing assembly by hand.</p> </li> <li> <p>Peter Hinch\u2019s Assembly FFT Library: For an authoritative, ready-made solution, Peter Hinch\u2019s <code>micropython-fourier</code> library is highly recommended. This is a MicroPython library implementing a fast FFT (actually a DFT class) almost entirely in ARM assembly, specifically tuned for boards with an FPU (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). It was written for the Pyboard (STM32F7) and has been updated to support the Pico 2\u2019s M33 core. The library computes single-precision FFTs in-place, uses precomputed twiddle factors, and does no heap allocation, meaning it can even be called in an interrupt handler for real-time use (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). It also offers conveniences like window functions and conversion to magnitude or dB. According to the author\u2019s benchmarks, a 1024-point FFT takes ~6.97\u00a0ms on the Pico 2 (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). Even a 256-point FFT would be only a fraction of that (the Reddit post below measured ~17\u00a0ms for 256-point including overhead in Python) (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). Hinch\u2019s code uses the M33\u2019s FPU for arithmetic, so it is floating-point based (more dynamic range than fixed-point). The repository includes documentation (README with usage notes) and a test script. To use it, you can copy the <code>.py</code> files to your Pico and import the <code>DFT</code> class in MicroPython. This gives you an object where you can populate data and run <code>fft()</code> to get the spectrum. (If you want to integrate with live ADC data, you\u2019d sample into a Python array or <code>array.array('f')</code>, then call the assembly routine to transform it.)</p> <p>Reference: \u201cFast Fourier transform in MicroPython\u2019s inline ARM assembler\u201d by Peter Hinch (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) \u2013 This is the library\u2019s README, which explains the design and usage. It explicitly notes it \u201crequires an ARM platform with FPU supporting Arm Thumb V7 assembler (e.g. Pyboard D, Pico\u00a02)\u201d (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). In other words, it\u2019s tailored for the Cortex-M4/M7/M33 class of devices. The README also provides a performance section comparing runtime on different boards (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.), confirming the Pico 2\u2019s advantage thanks to its 2\u00d7 clock and FPU/DSP support. </p> </li> <li> <p>MicroPython Dual-Core: It\u2019s worth noting that MicroPython on RP2040 currently runs on a single core by default. Using the second core is non-trivial in MicroPython (there is no built-in thread or task offload to the second core in the high-level API as of now). Advanced users have experimented with launching assembly code on the second core (via the <code>machine.mem</code> and writing to the second core\u2019s start address), but this is complex. If dual-core operation is a requirement, writing that portion in C (and maybe invoking it via MicroPython\u2019s FFI) or using an RTOS might be easier. However, given the Pico 2\u2019s speed, many audio FFT applications can be done on one core in MicroPython if the heavy math is in optimized assembly. For instance, user Dan McCreary demonstrated a MicroPython spectrum analyzer on the Pico 2: using an I2S microphone at 8 kHz, a 256-point FFT in assembly (~17\u00a0ms), and updating an OLED display (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico) (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). His results showed the FFT using only ~22% of the frame time (with the display update taking more time), so the optimized routine left plenty of CPU headroom (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). This suggests that offloading the math to assembly was enough to meet real-time requirements without needing the second core in MicroPython. Documentation for his project is available on his \u201cLearning MicroPython\u201d site, and it specifically credits Peter Hinch\u2019s FFT library for the speedup (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico) (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). </p> </li> </ul>"},{"location":"research/#conclusion-and-further-resources","title":"Conclusion and Further Resources","text":"<p>In summary, the Raspberry Pi Pico 2\u2019s RP2350 chip brings substantial DSP capability through its dual M33 cores with FPU/DSP extensions. To exploit this for fast FFTs: </p> <ul> <li>Use optimized libraries like CMSIS-DSP for C/C++ projects \u2013 these give you proven, assembly-tuned FFT functions that harness the hardware (floating or fixed-point) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). The official ARM docs and Raspberry Pi\u2019s examples (e.g. CMSIS integration in the SDK (New RP2040 CMSIS Pack - Raspberry Pi) or Arduino\u2019s CMSIS-DSP package) can guide you through setup. </li> <li>Study real-time examples (like Iwatake\u2019s dual-core analyzer (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io) or Bodmer\u2019s DMA FFT display (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT)) to understand how to pipeline data acquisition and processing. They illustrate techniques like double-buffering ADC data with DMA, using one core per task, and optimizing I/O throughput \u2013 all relevant to high-performance audio applications on the Pico 2. </li> <li>Leverage MicroPython\u2019s extensibility if you prefer Python at a high level. MicroPython by itself is not fast enough for DSP, but by injecting assembly (either inline or via a library) you can achieve performance close to a C implementation. Peter Hinch\u2019s FFT library is an excellent, well-documented starting point for MicroPython (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). Additionally, the <code>ulab</code> module can handle many numeric tasks in C for you, including FFT, which might simplify development for prototyping.</li> </ul> <p>By combining these resources, you can capture audio in real-time, compute an FFT with minimal latency, and display the spectrum \u2013 fully utilizing the Pico 2\u2019s hardware acceleration. Be sure to refer to the official Raspberry Pi Pico C/C++ SDK documentation for details on using the ADC, DMA, PIO, and multicore features, as those will be the backbone for any high-speed real-time processing on this platform. With the right approach, the tiny $5 Pico\u00a02 can perform serious signal processing tasks that were previously possible only on much more expensive or power-hungry systems.</p> <p>Sources: Documentation and code from Raspberry Pi Foundation and ARM, and community projects and libraries on GitHub: RP2350 hardware specs (Introducing the RP2350 | Stephen Smith's Blog), ARM CMSIS-DSP library API (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico), dual-core FFT example (iwatake) (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io), MicroPython assembly FFT by Peter Hinch (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.), and others as cited above.</p>"},{"location":"lessons/","title":"Spectrum Analyzer","text":"<ul> <li>Upper left is peak frequency 1,250 Hz</li> <li>Right is max frequency</li> <li>Frequency resolution: 31.25 Hz per bin</li> <li>Display frequency range: 0 Hz to 2656 Hz</li> <li>FFT size: 512</li> <li>Sample rate: 16,000 Hz</li> <li>I am whistling a tone at 1,250 Hz</li> <li>Microphone: INMP445</li> </ul>"},{"location":"lessons/#chatgpt-prompt","title":"ChatGPT Prompt","text":"<p>Prompt</p> <p>I would like to create an audio spectrum analyzer using a Raspberry Pi Pico 2 and a 128x64 OLED display using MicroPython. How would you suggest I go about this project. Break it down step-by-step.</p> <p>I started this project using a low-cost sound sensor below, but then I upgraded to the INMP445 on the I2S bus after seeing many problems with noise on the ADC.</p> <p> </p>"},{"location":"lessons/#chatgpt-gpt-4-response","title":"ChatGPT GPT-4 Response","text":"<p>Creating an audio spectrum analyzer with a Raspberry Pi Pico and a 128x64 OLED display using MicroPython is a fascinating project that combines hardware interfacing with digital signal processing. Below is a step-by-step guide to help you get started on this project.</p>"},{"location":"lessons/#step-1-gather-the-components","title":"Step 1: Gather the Components","text":"<ol> <li>Raspberry Pi Pico: A microcontroller board based on the RP2040 microcontroller chip.</li> <li>128x64 OLED Display: This will be used for visualizing the audio spectrum. Ensure it's compatible with the Pi Pico, ideally one that communicates over I2C or SPI.</li> <li>Electret Microphone Amplifier: This captures the audio signals. A module like the MAX4466 or a similar one would be suitable.</li> <li>Jump Wires: For connecting the components.</li> <li>Breadboard: Useful for prototyping without soldering.</li> <li>USB Cable: For programming the Raspberry Pi Pico.</li> </ol>"},{"location":"lessons/#step-2-set-up-the-raspberry-pi-pico","title":"Step 2: Set Up the Raspberry Pi Pico","text":"<ol> <li>Install MicroPython: Download and flash MicroPython onto the Raspberry Pi Pico. The Raspberry Pi Pico documentation provides clear instructions for this process.</li> <li>Prepare Your Development Environment: Install any necessary drivers and a code editor that supports MicroPython, such as Thonny or Visual Studio Code with the appropriate extensions.</li> </ol>"},{"location":"lessons/#step-3-connect-the-hardware","title":"Step 3: Connect the Hardware","text":"<ol> <li>Microphone to Pi Pico: Connect the output of the electret microphone amplifier to one of the analog input pins of the Pi Pico. Also, connect the ground and Vcc pins accordingly.</li> <li>OLED Display to Pi Pico: Connect the OLED display to the Pi Pico using I2C or SPI, depending on your display module. For I2C, you'll need to connect SDA, SCL, Vcc, and GND. For SPI, the connections will include SCK, MOSI, RES, DC, CS, Vcc, and GND.</li> </ol>"},{"location":"lessons/#step-4-write-the-software","title":"Step 4: Write the Software","text":"<ol> <li>Initialize the OLED Display: Write or adapt a MicroPython script to initialize the OLED display using the appropriate library. There are libraries available for both I2C and SPI OLED displays in MicroPython.</li> <li>Capture Audio Data: Write a script to capture audio data from the microphone using the ADC (Analog to Digital Converter) of the Pi Pico. The data capture should be done in real-time, and you may need to experiment with sampling rates.</li> <li>Process Audio Data: Implement a Fast Fourier Transform (FFT) algorithm to convert the time-domain audio signals into the frequency domain. This step is crucial for creating the spectrum analysis.</li> <li>Visualize the Spectrum: Map the frequency domain data to the OLED display. You will likely need to scale the data to fit nicely on the 128x64 display, representing different frequencies and their intensities.</li> </ol>"},{"location":"lessons/#step-5-testing-and-troubleshooting","title":"Step 5: Testing and Troubleshooting","text":"<ol> <li>Run Your Script: Test the entire system by running your script. You may need to adjust the audio capture settings or the way the data is visualized on the OLED display.</li> <li>Troubleshoot: If you encounter issues, use serial print statements to debug and understand where things might be going wrong.</li> </ol>"},{"location":"lessons/#step-6-refinement-and-expansion","title":"Step 6: Refinement and Expansion","text":"<ol> <li>Refine the Display: You can refine how the audio spectrum is displayed, perhaps by adjusting the resolution, colors (if your OLED supports it), or adding additional visual elements like peak hold.</li> <li>Experiment with Features: Consider adding features such as different modes for the spectrum analyzer, audio input from different sources, or even a GUI for interacting with the device.</li> </ol>"},{"location":"lessons/#resources","title":"Resources","text":"<ul> <li>MicroPython Documentation: Essential for understanding how to program the Raspberry Pi Pico.</li> <li>MicroPython FFT Libraries: Look for MicroPython libraries or examples of FFT implementations suitable for the Raspberry Pi Pico.  For example Peter Hinch's FFT Libaray on GitHub.  Peter has a Single precision FFT written in ARM assembler.</li> <li>OLED Libraries: Find libraries specific to your OLED display model for easier programming.</li> <li>ADC Sampling and FFT on Raspberry Pi Pico on Alex Wulff on hackster.io</li> </ul> <p>This project involves a mix of hardware setup, digital signal processing, and real-time data visualization. Take it step by step, and don't hesitate to seek out community forums and resources if you encounter challenges.</p>"},{"location":"lessons/01-test-incoming-sound/","title":"Test Incoming Sound","text":""},{"location":"lessons/01-test-incoming-sound/#prompt","title":"Prompt","text":"<p>Prompt</p> <p>OK, I now have a Raspberry Pi Pico and the OLED display hooked up.  I also have a microphone hooked up to ADC0 on port GPIO26.  How can I test that the signal is coming into the ADC?  Can you write a MicroPython program that displays the audio signal on the OLED?</p> <p>I am using a ssd1306 and I use the following MicroPython:</p> <pre><code>from machine import Pin\nimport ssd1306\n\nWIDTH = 128\nHEIGHT = 64\n\nclock=Pin(2) #SCL\ndata=Pin(3) #SDA\nRES = machine.Pin(4)\nDC = machine.Pin(5)\nCS = machine.Pin(6)\n\nspi=machine.SPI(0, sck=clock, mosi=data)\ndisplay = ssd1306.SSD1306_SPI(WIDTH, HEIGHT, spi, DC, RES, CS)\n# clear the display\ndisplay.fill(0)\n# draw a diagonal line\ndisplay.line(0, 0, WIDTH, HEIGHT, 1)\n</code></pre>"},{"location":"lessons/01-test-incoming-sound/#gpt-4-response","title":"GPT-4 Response","text":"<p>To test that the signal is coming into the ADC (Analog-to-Digital Converter) on the Raspberry Pi Pico via GPIO26, and to display the audio signal on your SSD1306 OLED, you can use the following MicroPython script. This script will read the analog values from the microphone connected to GPIO26 and plot these values in real-time on the OLED display, creating a simple waveform visualization.</p> <p>Note: Before running this script, ensure you have properly initialized and connected your microphone to GPIO26 (ADC0), and the OLED display is correctly wired for SPI communication as per your provided initialization.</p> <pre><code>import machine\nimport utime\nfrom machine import Pin, ADC, SPI\nimport ssd1306\n\n# OLED display width and height\nWIDTH = 128\nHEIGHT = 64\n\n# SPI pins for OLED\nclock = Pin(2) # SCL\ndata = Pin(3) # SDA\nRES = Pin(4)\nDC = Pin(5)\nCS = Pin(6)\n\n# Initialize SPI and OLED Display\nspi = SPI(0, sck=clock, mosi=data)\ndisplay = ssd1306.SSD1306_SPI(WIDTH, HEIGHT, spi, DC, RES, CS)\n\n# Initialize ADC for GPIO26 (ADC0)\nadc = ADC(Pin(26))\n\ndef plot_signal():\n    display.fill(0) # Clear the display\n    old_x = 0\n    old_y = HEIGHT // 2\n\n    # For simplicity, we're plotting every other pixel\n    for x in range(0, WIDTH, 2):\n        # Read from ADC (values will be from 0 to 4095)\n        val = adc.read_u16()\n        # Scale the ADC value to fit the OLED height\n        y = int((val / 65535) * HEIGHT)\n        # Invert y to plot correctly on the OLED\n        y = HEIGHT - y\n        # Draw a line from the last point to the new point\n        display.line(old_x, old_y, x, y, 1)\n        old_x, old_y = x, y\n\n    display.show() # Update the display with the new data\n\nwhile True:\n    plot_signal()\n    utime.sleep(0.1) # Small delay to reduce flickering\n</code></pre> <p>This script continuously reads the analog signal from the microphone and plots a simplified version of the waveform across the OLED display. Here's a breakdown of the plotting part:</p> <ul> <li>The screen is cleared at the beginning of each frame with <code>display.fill(0)</code>.</li> <li>We read a value from the ADC and scale it to fit the OLED's screen height.</li> <li>We draw a line from the previous point to the current point to create a continuous waveform.</li> <li>The display is updated with <code>display.show()</code> to visualize the waveform.</li> </ul> <p>This example provides a basic way to visualize audio signals. The resolution and quality of the visualization can be further enhanced or adjusted according to your requirements.</p>"},{"location":"lessons/01-test-incoming-sound/#generating-a-sine-wave-tone","title":"Generating a Sine Wave Tone","text":"<p>You are run the following p5.js MicroSim to play a tone from 0 to 4K kHz:</p> <p>MicroSim Tone Generator</p> <p>As you move the slider, the image on the display should change.</p>"},{"location":"lessons/03-display-scroll/","title":"Faster Display With Scroll","text":""},{"location":"lessons/03-display-scroll/#prompt","title":"Prompt","text":"<p>Prompt</p> <p>The display is a bit slow. Can you rewrite the plot_signal_with_gain function so that the scroll() function is used to move the display to the left by one pixel?</p>"},{"location":"lessons/03-display-scroll/#chatgpt-gpt-4-response","title":"ChatGPT GPT-4 Response","text":"<p>To make the display update faster and to create a scrolling effect that moves the waveform to the left by one pixel, you can modify the <code>plot_signal_with_gain</code> function to use the <code>scroll()</code> method of the SSD1306 library. This will shift the entire display content to the left by one pixel, and then you can plot the new data point at the right edge of the display. Here's how you can implement this:</p> <pre><code>def plot_signal_with_gain():\n    # Scroll the display content to the left by one pixel\n    display.scroll(-1, 0)\n\n    # Read gain control (potentiometer) value\n    gain_value = gain.read_u16() + 1  # Adding 1 to avoid division by zero\n\n    # Read from ADC (sound input)\n    val = sound.read_u16()\n\n    # Adjust the sound value based on the gain\n    adjusted_val = min(((val * gain_value) &gt;&gt; 16), 65535)  # Ensure the adjusted value does not exceed ADC's max value\n\n    # Scale the adjusted value to fit the OLED height\n    y = int((adjusted_val / 65535) * HEIGHT)\n    # Invert y to plot correctly on the OLED\n    y = HEIGHT - y\n\n    # Since we've scrolled, we only need to update the last column (x = WIDTH - 1)\n    # Clear the last column first (optional, depending on the desired visual effect)\n    for clear_y in range(HEIGHT):\n        display.pixel(WIDTH - 1, clear_y, 0)\n\n    # Draw the new data point at the right edge of the display\n    display.pixel(WIDTH - 1, y, 1)\n\n    # Update the display with the new data\n    display.show()\n</code></pre> <p>In this updated function, the display is scrolled left by one pixel at the beginning of each update cycle using <code>display.scroll(-1, 0)</code>. This creates room to draw the new data point at the far right edge (<code>WIDTH - 1</code>) of the display. The loop that clears the last column before drawing the new point is optional and can be adjusted or removed depending on the visual effect you're aiming for.</p> <p>Remember, while <code>scroll()</code> improves the visual fluidity by creating a continuous scrolling effect, the update rate can still be limited by the display's physical refresh rate and the speed at which your MicroPython script can run. Experiment with the <code>utime.sleep(0.1)</code> delay at the end of the main loop to find a balance between smoothness and speed that works for your specific application.</p>"},{"location":"lessons/03-display-scroll/#problems","title":"Problems","text":"<p>On the SSD1306 driver, there was no speedup since the display.show() sent the entire frame buffer. This step was much slower then pixel updates in the frame buffer.</p>"},{"location":"lessons/04-optimizing-fft-speed/","title":"Optimizing FFT (Part 1)","text":"<p>Prompt</p> <p>Please consider the programs in src/kits/fft. Can we create any optimizations to make the FFT go faster?  Can we precompute any numbers to make the calculation more efficient?  I am running this on a Raspberry Pi Pico 2 with Free memory: 470.83 KB</p>"},{"location":"lessons/04-optimizing-fft-speed/#claude-sonnet-37-response","title":"Claude Sonnet 3.7 Response","text":"<p>I've analyzed your 14-display-fft.py code and can suggest several optimizations to make the FFT calculations faster on your Raspberry Pi Pico 2. The current implementation of the Cooley-Tukey FFT algorithm is recursive and creates many temporary lists, which is inefficient for microcontrollers with limited resources.</p> <p>Here are optimizations we can implement:</p> <ol> <li>Replace the recursive FFT with an iterative in-place algorithm</li> <li>Precompute the twiddle factors (the complex exponentials)</li> <li>Use bit-reversal for the initial reordering</li> <li>Implement integer-based calculations where possible</li> <li>Optimize the windowing function</li> </ol> <p>The optimizations I've implemented in the code above include:</p>"},{"location":"lessons/04-optimizing-fft-speed/#1-precomputed-arrays","title":"1. Precomputed Arrays","text":"<ul> <li>Hanning window coefficients are calculated once at startup</li> <li>Bit-reversal table for the FFT reordering</li> <li>Twiddle factors (complex exponentials) are precomputed and stored</li> </ul>"},{"location":"lessons/04-optimizing-fft-speed/#2-in-place-iterative-fft","title":"2. In-place Iterative FFT","text":"<ul> <li>Replaced the recursive algorithm with an iterative version</li> <li>Eliminates the need for temporary arrays during computation</li> <li>Uses in-place calculations to minimize memory usage</li> </ul>"},{"location":"lessons/04-optimizing-fft-speed/#3-fast-magnitude-calculation","title":"3. Fast Magnitude Calculation","text":"<ul> <li>Added a fast approximation for the magnitude calculation</li> <li>Uses |z| \u2248 max(|Re(z)|, |Im(z)|) + 0.4 * min(|Re(z)|, |Im(z)|) instead of the more expensive square root</li> </ul>"},{"location":"lessons/04-optimizing-fft-speed/#4-memory-reuse","title":"4. Memory Reuse","text":"<ul> <li>Reuses the same arrays for FFT calculation across iterations</li> <li>Avoids memory fragmentation and garbage collection delays</li> </ul>"},{"location":"lessons/04-optimizing-fft-speed/#5-efficient-data-structures","title":"5. Efficient Data Structures","text":"<ul> <li>Uses typed arrays (<code>array.array</code>) for all numerical data</li> <li>Provides better performance than regular Python lists</li> </ul>"},{"location":"lessons/04-optimizing-fft-speed/#6-optimized-display-rendering","title":"6. Optimized Display Rendering","text":"<ul> <li>Simplified the bin averaging logic</li> <li>More efficient bounds checking</li> </ul> <p>These optimizations should significantly improve the performance of your FFT calculations on the Raspberry Pi Pico 2. The pre-computation step may use a bit of memory initially, but it will save substantial computation time during the main loop.</p> <p>A few additional notes:</p> <ul> <li>The code prioritizes speed over memory usage, which is appropriate given your available memory (470.83 KB)</li> <li>The bit-reversal pre-computation will speed up the FFT initialization stage</li> <li>If memory becomes an issue, you could reduce FFT_SIZE to 256, which would still provide reasonable frequency resolution</li> </ul> <p>I've fixed the MicroPython compatibility issue in the bit-reverse function. The previous version used string slicing with a negative step (<code>[::-1]</code>), which isn't supported in MicroPython. The new version uses bitwise operations instead:</p> <pre><code>def bit_reverse(n, bits):\n    result = 0\n    for i in range(bits):\n        result = (result &lt;&lt; 1) | (n &amp; 1)\n        n &gt;&gt;= 1\n    return result\n</code></pre> <p>This iterative approach manipulates the bits directly, shifting and masking to reverse the bit order.</p> <p>The optimizations in this version include:</p> <ol> <li> <p>MicroPython-compatible bit reversal - Using bitwise operations instead of string manipulation</p> </li> <li> <p>Precomputed tables:</p> </li> <li> <p>Hanning window coefficients</p> </li> <li>Bit-reversal indices</li> <li> <p>Twiddle factors (complex exponentials)</p> </li> <li> <p>In-place FFT algorithm - This reduces memory usage and avoids allocations during the FFT calculation</p> </li> <li> <p>Optimized magnitude calculation - Using a faster approximation: <code>max(|re|, |im|) + 0.4 * min(|re|, |im|)</code></p> </li> <li> <p>Memory reuse - Arrays are allocated once and reused across iterations</p> </li> </ol> <p>This optimized version should run significantly faster than the original implementation while being fully compatible with MicroPython on your Raspberry Pi Pico 2.</p>"},{"location":"lessons/05-frequency-shift/","title":"Frequency Shift","text":"<p>Prompt</p> <p>This is working better.  I have one additional request.  To give demos of the FFT working I am whistling into the microphone.  As I change my pitch I want to see the spike move.  But my whistling range is mostly on the left side of the display.  Can we change the display so that the lower frequencies are shifted to the right by about 1/3 the screen size.  I realize this will not show the higher frequencies to the right, but that is OK.</p> <p>That's a great request! Let me modify the code to focus on the lower frequencies by shifting the display. This will make your whistling demonstrations more effective by using more screen space for the frequencies you're actually working with.</p> <p>I've modified the code to focus on the lower frequencies where your whistling occurs. Here's what I changed:</p> <ol> <li> <p>Instead of displaying the entire frequency spectrum, the display now shows only the first third of the frequency bins (where your whistling will be detected).</p> </li> <li> <p>The code computes:</p> </li> </ol> <pre><code>lower_freq_focus = len(magnitudes) // 3\n</code></pre> <p>This means we're only using approximately the first 85 frequency bins (out of 256) and spreading them across the entire display width.</p> <ol> <li>The bin width calculation is updated to use only this lower frequency portion:</li> </ol> <pre><code>bin_width = lower_freq_focus // num_bins\n</code></pre> <ol> <li>The main for-loop now limits the frequency analysis to this lower range.</li> </ol> <p>This change will effectively \"zoom in\" on the lower frequency range where your whistling occurs, making the movement of peaks much more noticeable as you change pitch. The entire display width now represents only the first third of the frequency spectrum, giving you about 3 times more horizontal resolution for your demonstrations.</p>"},{"location":"lessons/05-frequency-shift/#possible-next-steps","title":"Possible Next Steps:","text":"<ul> <li>Adjust the frequency range further (more or less than 1/3)</li> <li>Apply a logarithmic scale to the frequency display to better match human hearing perception</li> <li>Highlight the peaks more prominently and display the frequencies of the peaks</li> </ul>"},{"location":"lessons/06-peak-frequency/","title":"Peak Frequency","text":"<p>Prompt</p> <p>Please display the frequency that has the highest value on the upper left.  Don't display the \"Hz\", just the number.</p> <p>I'll modify the display to show the frequency with the highest value in the upper left instead of 0Hz, and remove the \"Hz\" from both labels to keep them clean and simple.</p> <p>Optimized FFT Implementation for MicroPython</p> <p>I've updated the display to show the peak frequency in the upper left corner instead of 0Hz, and I've removed the \"Hz\" label from both numbers as requested.</p> <p>Here's what the new code does:</p> <ol> <li>Finds the peak frequency:</li> </ol> <pre><code># Find the frequency bin with the highest magnitude\nmax_idx = 0\nmax_val = 0\nfor i in range(lower_freq_focus):\n    if i &lt; len(magnitudes) and magnitudes[i] &gt; max_val:\n        max_val = magnitudes[i]\n        max_idx = i\n</code></pre> <ol> <li>Calculates the actual frequency in Hz:</li> </ol> <pre><code># Calculate the peak frequency in Hz\npeak_freq =int(max_idx * bin_freq_width)`\n</code></pre>"},{"location":"lessons/06-peak-frequency/#full-program","title":"Full Program","text":"<pre><code># Sound Spectrum Analyzer with FFT - Optimized for MicroPython\n# Combines INMP441 I2S microphone with SSD1306 OLED display and performs FFT\nfrom machine import I2S, Pin, SPI\nimport ssd1306\nimport math\nimport struct\nimport time\nimport array\n\n# OLED Display configuration\nSCL = Pin(2)  # SPI Clock\nSDA = Pin(3)  # SPI Data\nRES = Pin(4)  # Reset\nDC = Pin(5)   # Data/Command\nCS = Pin(6)   # Chip Select\n\n# Initialize SPI and OLED\nspi = SPI(0, sck=SCL, mosi=SDA)\noled = ssd1306.SSD1306_SPI(128, 64, spi, DC, RES, CS)\n\n# I2S Microphone configuration\nSCK_PIN = 10  # Serial Clock\nWS_PIN = 11   # Word Select\nSD_PIN = 12   # Serial Data\n\n# I2S configuration parameters\nI2S_ID = 0\nSAMPLE_SIZE_IN_BITS = 32\nFORMAT = I2S.MONO\nSAMPLE_RATE = 16000\nBUFFER_LENGTH_IN_BYTES = 40000\n\n# Initialize I2S for microphone\naudio_in = I2S(\n    I2S_ID,\n    sck=Pin(SCK_PIN),\n    ws=Pin(WS_PIN),\n    sd=Pin(SD_PIN),\n    mode=I2S.RX,\n    bits=SAMPLE_SIZE_IN_BITS,\n    format=FORMAT,\n    rate=SAMPLE_RATE,\n    ibuf=BUFFER_LENGTH_IN_BYTES,\n)\n\n# FFT size (must be a power of 2)\nFFT_SIZE = 512\n\n# Precompute the Hanning window coefficients\nhanning_window = array.array('f', [0] * FFT_SIZE)\nfor i in range(FFT_SIZE):\n    hanning_window[i] = 0.5 * (1 - math.cos(2 * math.pi * i / (FFT_SIZE - 1)))\n\n# Precompute bit-reversal table - MicroPython compatible version\ndef bit_reverse(n, bits):\n    result = 0\n    for i in range(bits):\n        result = (result &lt;&lt; 1) | (n &amp; 1)\n        n &gt;&gt;= 1\n    return result\n\nbit_reverse_table = array.array('H', [0] * FFT_SIZE)\nbits = int(math.log2(FFT_SIZE))\nfor i in range(FFT_SIZE):\n    bit_reverse_table[i] = bit_reverse(i, bits)\n\n# Precompute twiddle factors (complex exponentials)\ntwiddle_factors_real = array.array('f', [0] * (FFT_SIZE // 2))\ntwiddle_factors_imag = array.array('f', [0] * (FFT_SIZE // 2))\nfor i in range(FFT_SIZE // 2):\n    angle = -2 * math.pi * i / FFT_SIZE\n    twiddle_factors_real[i] = math.cos(angle)\n    twiddle_factors_imag[i] = math.sin(angle)\n\ndef capture_audio_samples():\n    \"\"\"Capture audio samples for FFT processing\"\"\"\n    # For 32-bit samples, we need 4 bytes per sample\n    NUM_SAMPLE_BYTES = FFT_SIZE * 4\n\n    # Raw samples will be stored in this buffer (signed 32-bit integers)\n    samples_raw = bytearray(NUM_SAMPLE_BYTES)\n\n    # Read samples from I2S microphone\n    num_bytes_read = audio_in.readinto(samples_raw)\n\n    if num_bytes_read == 0:\n        return None\n\n    # Process raw samples\n    format_str = \"&lt;{}i\".format(num_bytes_read // 4)\n    samples = struct.unpack(format_str, samples_raw[:num_bytes_read])\n\n    # Convert to float array and apply windowing function\n    # Reuse the same arrays to avoid memory allocation\n    real = array.array('f', [0] * len(samples))\n    imag = array.array('f', [0] * len(samples))\n\n    for i in range(len(samples)):\n        # Shift right by 8 bits for INMP441 and apply window\n        real[i] = (samples[i] &gt;&gt; 8) * hanning_window[i]\n        imag[i] = 0  # Imaginary part is zero for real input\n\n    return (real, imag)\n\ndef iterative_fft(real, imag):\n    \"\"\"Compute FFT in-place using an iterative algorithm\"\"\"\n    n = len(real)\n    assert n == len(imag)\n    assert n == FFT_SIZE, \"FFT size must match precomputed tables\"\n\n    # Bit-reverse reordering\n    for i in range(n):\n        j = bit_reverse_table[i]\n        if i &lt; j:\n            real[i], real[j] = real[j], real[i]\n            imag[i], imag[j] = imag[j], imag[i]\n\n    # Cooley-Tukey iterative FFT\n    for stage in range(1, int(math.log2(n)) + 1):\n        m = 2 ** stage  # Distance between butterflies\n        m2 = m // 2     # Distance between butterfly pairs\n\n        # Process each group of butterflies\n        for k in range(0, n, m):\n            # Process each butterfly pair\n            for j in range(m2):\n                # Get twiddle factor\n                twiddle_idx = j * (n // m)\n                wr = twiddle_factors_real[twiddle_idx]\n                wi = twiddle_factors_imag[twiddle_idx]\n\n                # Get indices for butterfly operation\n                idx1 = k + j\n                idx2 = idx1 + m2\n\n                # Calculate butterfly\n                tr = real[idx2] * wr - imag[idx2] * wi\n                ti = real[idx2] * wi + imag[idx2] * wr\n\n                # Store results\n                real[idx2] = real[idx1] - tr\n                imag[idx2] = imag[idx1] - ti\n                real[idx1] = real[idx1] + tr\n                imag[idx1] = imag[idx1] + ti\n\n    return (real, imag)\n\ndef calculate_magnitudes(real, imag):\n    \"\"\"Calculate magnitude spectrum from complex FFT result\"\"\"\n    # Only need the first half due to symmetry for real input\n    mags = array.array('f', [0] * (FFT_SIZE // 2))\n\n    # Use a fast approximation for magnitude calculation\n    # |z| \u2248 max(|Re(z)|, |Im(z)|) + 0.4 * min(|Re(z)|, |Im(z)|)\n    for i in range(FFT_SIZE // 2):\n        re_abs = abs(real[i])\n        im_abs = abs(imag[i])\n        if re_abs &gt; im_abs:\n            mags[i] = re_abs + 0.4 * im_abs\n        else:\n            mags[i] = im_abs + 0.4 * re_abs\n\n    return mags\n\ndef draw_spectrum(magnitudes):\n    \"\"\"Draw the frequency spectrum on the OLED display with focus on lower frequencies\"\"\"\n    # Clear the display\n    oled.fill(0)\n\n    # Number of frequency bins to display\n    num_bins = 64\n\n    # Focus on lower frequencies by only taking the first third of the spectrum\n    # This will make whistling frequencies more visible across the display\n    lower_freq_focus = len(magnitudes) // 3\n\n    # Calculate the frequency range being displayed\n    # Nyquist frequency is half the sample rate\n    nyquist_freq = SAMPLE_RATE / 2\n    # Each bin represents nyquist_freq/FFT_SIZE Hz\n    bin_freq_width = nyquist_freq / (FFT_SIZE // 2)\n\n    # Calculate frequency range start and end\n    freq_start = 0  # Hz\n    freq_end = int(bin_freq_width * lower_freq_focus)  # Hz\n\n    # Combine frequency bins to fit display (use precomputed indexes)\n    display_bins = array.array('f', [0] * num_bins)\n    bin_width = lower_freq_focus // num_bins\n\n    # Simple averaging of bins, focusing on lower frequencies\n    for i in range(num_bins):\n        start_idx = i * bin_width\n        end_idx = min((i + 1) * bin_width, lower_freq_focus)\n        if start_idx &gt;= lower_freq_focus:\n            break\n\n        # Fast averaging\n        bin_sum = 0\n        for j in range(start_idx, end_idx):\n            bin_sum += magnitudes[j]\n        display_bins[i] = bin_sum / (end_idx - start_idx) if end_idx &gt; start_idx else 0\n\n    # Find maximum value for scaling (avoid division by zero)\n    max_magnitude = 1\n    for mag in display_bins:\n        if mag &gt; max_magnitude:\n            max_magnitude = mag\n\n    # Reserve top row for frequency labels\n    top_margin = 8\n\n    # Use display height minus top margin\n    display_height = 54  # 64 - top margin - 2\n    baseline = 63        # Start from the bottom of the screen\n    scaling_factor = 0.5  # Adjust scaling to distribute bars\n\n    # Find the frequency bin with the highest magnitude\n    max_idx = 0\n    max_val = 0\n    for i in range(lower_freq_focus):\n        if i &lt; len(magnitudes) and magnitudes[i] &gt; max_val:\n            max_val = magnitudes[i]\n            max_idx = i\n\n    # Calculate the peak frequency in Hz\n    peak_freq = int(max_idx * bin_freq_width)\n\n    # Display peak frequency on the left and max range on the right\n    oled.text(\"{}\".format(peak_freq), 0, 0, 1)\n    end_text = \"{}\".format(freq_end)\n    # Position end frequency text right-aligned\n    end_x = 128 - (len(end_text) * 8)  # Each character is ~8 pixels wide\n    oled.text(end_text, end_x, 0, 1)\n\n    # Draw the spectrum - each bin takes 2 pixels width\n    for i in range(len(display_bins)):\n        # Apply sqrt scaling for more balanced distribution\n        normalized = display_bins[i] / max_magnitude\n        # Use math.sqrt since we can't avoid it in MicroPython\n        height = int(math.sqrt(normalized) * display_height * scaling_factor)\n\n        # Draw vertical bar\n        x = i * 2  # Each bar is 2 pixels wide\n        for y in range(baseline, baseline - height, -1):\n            if y &gt;= top_margin:  # Ensure we don't write over the frequency text\n                oled.pixel(x, y, 1)\n                oled.pixel(x + 1, y, 1)  # Make bars 2 pixels wide\n\n    # Draw baseline\n    oled.hline(0, baseline, 128, 1)\n\n    # Update the display\n    oled.show()\n\ntry:\n    print(\"Optimized Sound Spectrum Analyzer\")\n    print(\"Press Ctrl+C to stop\")\n\n    # Calculate and print frequency range information\n    nyquist_freq = SAMPLE_RATE / 2\n    bin_freq_width = nyquist_freq / (FFT_SIZE // 2)\n    lower_freq_focus = (FFT_SIZE // 2) // 3\n    max_freq = int(bin_freq_width * lower_freq_focus)\n\n    print(\"Frequency resolution: {:.2f} Hz per bin\".format(bin_freq_width))\n    print(\"Display frequency range: 0 Hz to {} Hz\".format(max_freq))\n    print(\"FFT size: {}\".format(FFT_SIZE))\n    print(\"Sample rate: {} Hz\".format(SAMPLE_RATE))\n\n    # Allocate arrays once for reuse\n    real_buffer = array.array('f', [0] * FFT_SIZE)\n    imag_buffer = array.array('f', [0] * FFT_SIZE)\n\n    # Main processing loop\n    while True:\n        # Capture audio samples\n        samples = capture_audio_samples()\n\n        if samples:\n            # Get real and imaginary arrays\n            fft_real, fft_imag = samples\n\n            # Compute FFT in-place\n            iterative_fft(fft_real, fft_imag)\n\n            # Calculate magnitude spectrum\n            magnitudes = calculate_magnitudes(fft_real, fft_imag)\n\n            # Draw the spectrum\n            draw_spectrum(magnitudes)\n\nexcept KeyboardInterrupt:\n    print(\"Monitoring stopped\")\nfinally:\n    # Clean up\n    audio_in.deinit()\n    print(\"Program terminated\")\n</code></pre> <pre><code>Optimized Sound Spectrum Analyzer\nPress Ctrl+C to stop\nFrequency resolution: 31.25 Hz per bin\nDisplay frequency range: 0 Hz to 2656 Hz\nFFT size: 512\nSample rate: 16000 Hz\n</code></pre> <p>This enhancement makes the display more informative for your demonstrations. As you whistle, the number on the upper left will update to show the exact frequency of your current pitch, while the number on the right shows the maximum frequency displayed on the screen.</p> <p>This real-time peak frequency display is particularly useful for tuning instruments or demonstrating precise pitch control during your whistling demonstrations.</p>"},{"location":"lessons/07-speedups/","title":"Further Performance Improvements","text":"<p>Prompt</p> <p>The MicroPython frequency analyzer code in the file 20-fft-range-2600.py is working pretty well on the fast new Raspberry Pi Pico 2.  However, it has a noticeable lag between when a sound as a specific frequency is played and when the peak appears on the display.  The delay is a few seconds.  Can we lower the delay in any way?  Just look at the code and give me some options like using DMA or using a version of the FFT written in ARM assembly code.</p> <p>20-fft-range-2600.py in GitHub src/kits/fft</p>"},{"location":"lessons/07-speedups/#claude-sonnet-37-response","title":"Claude Sonnet 3.7 Response","text":"<p>I'll analyze the code to suggest ways to reduce the lag between sound input and display updates in your frequency analyzer. Here are several optimization options:</p>"},{"location":"lessons/07-speedups/#1-reduce-sample-size-and-processing","title":"1. Reduce Sample Size and Processing","text":"<p>The current FFT_SIZE of 512 provides good resolution but requires significant processing time. Consider:</p> <pre><code># Try a smaller FFT size\nFFT_SIZE = 256  # or even 128\n</code></pre> <p>This would cut processing time significantly at the cost of some frequency resolution.</p>"},{"location":"lessons/07-speedups/#2-optimize-memory-usage","title":"2. Optimize Memory Usage","text":"<p>The code creates new arrays frequently, which triggers garbage collection and slows things down:</p> <ul> <li>Use pre-allocated buffers more consistently</li> <li>Eliminate redundant array allocations in <code>capture_audio_samples()</code> and <code>calculate_magnitudes()</code></li> <li>Use a single shared array for the magnitudes</li> </ul>"},{"location":"lessons/07-speedups/#3-simplify-the-display-logic","title":"3. Simplify the Display Logic","text":"<p>The display rendering is relatively complex:</p> <ul> <li>Reduce number of bins displayed (try 32 instead of 64)</li> <li>Simplify scaling calculations</li> <li>Only update parts of the display that change, not the entire screen</li> </ul>"},{"location":"lessons/07-speedups/#4-hardware-acceleration-options","title":"4. Hardware Acceleration Options","text":"<p>For more significant improvements:</p>"},{"location":"lessons/07-speedups/#use-pio-for-i2s-data-collection","title":"Use PIO for I2S Data Collection","text":"<p>The Pico's PIO (Programmable I/O) can handle I2S sampling in the background while the CPU works on FFT processing:</p> <pre><code># Example of using PIO for I2S\nfrom machine import Pin\nimport rp2\n\n@rp2.asm_pio(\n    fifo_join=rp2.PIO.JOIN_RX,\n    in_shiftdir=rp2.PIO.SHIFT_LEFT,\n    autopush=True,\n    push_thresh=32\n)\ndef i2s_pio():\n    # PIO program for I2S capture\n    # ...\n</code></pre>"},{"location":"lessons/07-speedups/#use-dma-for-memory-transfers","title":"Use DMA for Memory Transfers","text":"<p>DMA could handle moving data from I2S to memory and from memory to display without CPU intervention:</p> <pre><code># Example DMA setup\nfrom machine import mem32\n\n# Set up DMA channel for data transfer\nDMA_BASE = 0x50000000\n# Configure DMA registers for transfer\n</code></pre>"},{"location":"lessons/07-speedups/#arm-dsp-assembly-optimizations","title":"ARM DSP Assembly Optimizations","text":"<p>For the RP2040 chip in the Pico, you could use optimized ARM assembly for FFT:</p> <ol> <li>Create a C module with ARM CMSIS DSP library for FFT</li> <li>Compile it as a MicroPython native module</li> <li>Import and use it in your Python code</li> </ol> <pre><code># Example usage of a hypothetical native FFT module\nimport armfft\n\n# Then replace your FFT function with:\ndef process_fft(samples):\n    return armfft.compute(samples)\n</code></pre>"},{"location":"lessons/07-speedups/#5-parallel-processing-with-dual-cores","title":"5. Parallel Processing with Dual Cores","text":"<p>The Pico has dual cores - you could dedicate one core to FFT processing and the other to display:</p> <pre><code>import _thread\n\n# Core 1: Handle audio sampling and FFT\ndef core1_task():\n    while True:\n        # Sample audio and compute FFT\n        # Place results in shared buffer\n\n# Core 0 (main): Handle display updates\n_thread.start_new_thread(core1_task, ())\nwhile True:\n    # Read from shared buffer\n    # Update display\n</code></pre>"},{"location":"lessons/07-speedups/#6-efficient-algorithm-implementation","title":"6. Efficient Algorithm Implementation","text":"<p>Replace the current iterative FFT with a more optimized version like the split-radix FFT or use a fixed-point implementation that avoids floating-point operations.</p>"},{"location":"lessons/07-speedups/#implementation-priority","title":"Implementation Priority","text":"<p>I recommend trying these options in this order:</p> <ol> <li>First, try reducing the FFT size to 256 or 128 - this is the simplest change with significant impact</li> <li>Next, implement dual-core processing to parallelize computation and display</li> <li>Then optimize memory usage with pre-allocated buffers</li> <li>Finally, consider PIO and DMA if further optimization is needed</li> </ol> <p>Would you like more details on implementing any of these specific approaches?</p>"},{"location":"lessons/07-speedups/#limited-frequency-analysis","title":"Limited Frequency Analysis","text":""},{"location":"lessons/07-speedups/#changing-the-fft-size","title":"Changing the FFT Size","text":"<p>Prompt</p> <p>For demos, you can see we are only looking at the frequencies below  2,600 Hz.  Will using 256 or 128 sizes still work?</p> <p>Yes, using FFT_SIZE of 256 or 128 would still work well for your frequency range of interest (below 2,600 Hz).</p> <p>Let's analyze why:</p> <p>With your current configuration:</p> <ul> <li>Sample rate: 16,000 Hz</li> <li>FFT_SIZE: 512</li> <li>Nyquist frequency: 8,000 Hz</li> <li>Frequency resolution: 31.25 Hz per bin (16,000/512 * 0.5)</li> <li>You're focusing on 0-2,600 Hz (roughly the first third of your spectrum)</li> </ul> <p>If you reduce to FFT_SIZE = 256:</p> <ul> <li>Frequency resolution becomes: 62.5 Hz per bin</li> <li>You'd still have about 41-42 bins for your 0-2,600 Hz range (2600/62.5)</li> <li>This is enough resolution for most audio demos showing voice, whistles, or musical tones</li> </ul> <p>If you reduce to FFT_SIZE = 128:</p> <ul> <li>Frequency resolution becomes: 125 Hz per bin</li> <li>You'd have about 20-21 bins for your 0-2,600 Hz range</li> <li>Still sufficient for showing major frequency peaks but with less precision</li> </ul> <p>The tradeoffs:</p> <p>FFT_SIZE = 256:</p> <ul> <li>Processing time reduced by approximately 50%</li> <li>Memory usage reduced by 50%</li> <li>Resolution still good enough to distinguish nearby frequencies</li> <li>Suitable for most demo purposes</li> </ul> <p>FFT_SIZE = 128:</p> <ul> <li>Processing time reduced by approximately 75%</li> <li>Memory usage reduced by 75% </li> <li>Resolution becomes more coarse but still usable</li> <li>Better for showing only major frequency components</li> </ul> <p>For demo purposes where responsiveness is more important than precision, I'd recommend trying FFT_SIZE = 256 first. This should significantly reduce your lag while still providing good visual feedback for frequency content below 2,600 Hz.</p> <p>If you're primarily interested in showing distinct tones (like whistling or musical notes) rather than detailed frequency analysis, even FFT_SIZE = 128 should be sufficient and provide very responsive updates.</p>"},{"location":"lessons/07-speedups/#lower-the-sample-rate","title":"Lower the Sample Rate","text":"<p>Prompt</p> <p>Would it help to change the sample rate from 16K down to 8K?</p> <p>Yes, reducing the sample rate from 16 kHz to 8 kHz would definitely help reduce processing lag, particularly for your specific use case.</p> <p>Let's analyze why:</p> <ol> <li> <p>Lower computational load:</p> </li> <li> <p>Half the samples to process means roughly half the processing time</p> </li> <li>The I2S data collection will require half the bandwidth</li> <li> <p>Less memory needed for buffers</p> </li> <li> <p>Still sufficient for your frequency range:</p> </li> <li> <p>With 8 kHz sample rate, your Nyquist frequency becomes 4 kHz</p> </li> <li>Since you're only interested in frequencies below 2.6 kHz, this is perfectly adequate</li> <li> <p>You're not losing any useful frequency information for your application</p> </li> <li> <p>Better frequency resolution trade-off:</p> </li> <li> <p>With 8 kHz sample rate and FFT_SIZE = 256:</p> <ul> <li>Frequency resolution would be 31.25 Hz (8000/256 * 0.5)</li> <li>This is the same resolution as your current setup with FFT_SIZE = 512 at 16 kHz</li> </ul> </li> <li>With 8 kHz sample rate and FFT_SIZE = 128:<ul> <li>Frequency resolution would be 62.5 Hz</li> <li>Still quite reasonable for visualizing frequency components in your range</li> </ul> </li> </ol> <p>By combining a lower sample rate (8 kHz) with a smaller FFT size (256 or 128), you could reduce the processing load by 75-87.5% compared to your current implementation, which should significantly reduce the lag between sound input and display updates.</p> <p>To implement this change, you would just need to update the sample rate in your I2S configuration:</p> <pre><code>SAMPLE_RATE = 8000  # Changed from 16000\n</code></pre> <p>And potentially adjust the FFT size:</p> <pre><code>FFT_SIZE = 256  # or 128 for even faster processing\n</code></pre> <p>This would be one of the most straightforward optimizations with a substantial impact on responsiveness.</p>"},{"location":"lessons/07-speedups/#using-assembly-version-of-fft-arm","title":"Using Assembly Version of FFT (ARM)","text":"<p>Prompt</p> <p>I have found a person that claims to have written a single precision FFT written in ARM assembler that is callable from MicroPython.  The GitHub site implies that the Pico 2 could use this.  Is it possible this would work?</p> <p>https://github.com/peterhinch/micropython-fourier/blob/master/README.md</p> <p>This looks very promising for your project. The micropython-fourier library by Peter Hinch is specifically designed for MicroPython and includes ARM assembly optimized FFT implementations. Let's analyze whether this would work for your Raspberry Pi Pico 2:</p> <ol> <li> <p>Compatibility:</p> </li> <li> <p>The README explicitly states it's compatible with the RP2040 (the chip in both Pico and Pico 2)</p> </li> <li> <p>It lists specific support for the Pico, and the Pico 2 uses the same architecture with higher clock speeds</p> </li> <li> <p>Performance benefits:</p> </li> <li> <p>The library claims significant speed improvements - up to 6x faster than equivalent Python code</p> </li> <li>Written in ARM assembly specifically optimized for the Cortex-M0+ (RP2040's architecture)</li> <li> <p>The benchmarks in the documentation show impressive performance gains</p> </li> <li> <p>Integration ease:</p> </li> <li> <p>Designed as a MicroPython module, making it straightforward to integrate with your existing code</p> </li> <li>Includes both FFT and inverse FFT implementations</li> <li>Supports single-precision floating-point operations</li> </ol> <p>To implement this in your project, you would:</p> <ol> <li>First install the library on your Pico 2:</li> </ol> <pre><code># Using mip (MicroPython package manager)\nimport mip\nmip.install(\"github:peterhinch/micropython-fourier\")\n</code></pre> <ol> <li>Then modify your code to use this optimized FFT:</li> </ol> <pre><code>from fourier import fft\n\n# Replace your iterative_fft function with this:\ndef process_fft(real, imag):\n    # The library might expect a different data format\n    # Check documentation for exact usage\n    return fft(real, imag)\n</code></pre> <p>The speed improvement from using assembly-optimized code, combined with the other optimizations we discussed (reduced sample rate and smaller FFT size), could dramatically reduce the lag in your frequency analyzer.</p> <p>Given that Peter Hinch is a well-known contributor to the MicroPython ecosystem and his libraries are generally high quality, this is likely your best option for optimizing FFT performance on the Pico 2.</p> <p>Challenges</p> <ul> <li>Try different window functions (like Hamming or Blackman windows instead of Hanning) for different spectral leakage characteristics</li> <li>Try Adjusting the scaling factor to get better visual representation of quiet sounds</li> <li>Try Creating different display modes (like a waterfall display or peak hold feature)</li> </ul>"},{"location":"lessons/08-adding-timers/","title":"Adding Timers","text":"<p>Prompt</p> <p>I want to know how much time iit takes to capture_audio_samples(), run the fft and then draw_spectrum().  Please put in code so that it will display the average times for these components in the console using print().  Pick a counter of about 100 to average the times in various sections of the code.</p>"},{"location":"lessons/08-adding-timers/#cursor-response","title":"Cursor Response","text":"<p>I'll help you add timing measurements to analyze the performance of different components of your sound spectrum analyzer. I'll modify the code to measure and display average times for capturing audio samples, running the FFT, and drawing the spectrum.</p> <p>I've modified the code to add timing measurements for each major component of the processing pipeline. Here's what I've added:</p> <p>Let me edit the file to add timing measurements:</p> <p>I've modified the code to add timing measurements for each major component of the processing pipeline. Here's what I've added</p> <p>Three arrays to store timing measurements for 100 iterations:</p> <pre><code>  # Initialize timing counters\n    capture_times = array.array('f', [0] * 100)\n    fft_times = array.array('f', [0] * 100)\n    draw_times = array.array('f', [0] * 100)\n    counter = 0\n</code></pre> <p>capture_times for audio capture</p> <pre><code>capture_start = time.ticks_us()\nsamples = capture_audio_samples()\ncapture_end = time.ticks_us()\n</code></pre> <p>fft_times for FFT processing:</p> <pre><code># Compute FFT in-place with timing\nfft_start = time.ticks_us()\niterative_fft(fft_real, fft_imag)\nfft_end = time.ticks_us()\n</code></pre> <p>draw_times for spectrum drawing</p> <pre><code># Draw the spectrum with timing\ndraw_start = time.ticks_us()\ndraw_spectrum(magnitudes)\ndraw_end = time.ticks_us()\n</code></pre> <p>Timing measurements using time.ticks_us() for microsecond precision:</p> <p>Display all metrics in the console</p> <pre><code>counter += 1\nif counter &gt;= 100:\n    # Calculate averages\n    avg_capture = sum(capture_times) / 100\n    avg_fft = sum(fft_times) / 100\n    avg_draw = sum(draw_times) / 100\n    total_time = avg_capture + avg_fft + avg_draw\n\n    print(\"\\nPerformance Metrics (averaged over 100 iterations):\")\n    print(f\"Capture time: {avg_capture:.2f} ms\")\n    print(f\"FFT time: {avg_fft:.2f} ms\")\n    print(f\"Draw time: {avg_draw:.2f} ms\")\n    print(f\"Total processing time: {total_time:.2f} ms\")\n    print(f\"Frames per second: {1000/total_time:.1f}\")\n\n    # Reset counter\n    counter = 0\n</code></pre>"},{"location":"lessons/08-adding-timers/#sample-results","title":"Sample Results","text":"<pre><code>Performance Metrics (averaged over 100 iterations):\nCapture time: 6.97 ms\nFFT time: 101.65 ms\nDraw time: 33.58 ms\nTotal processing time: 142.20 ms\nFrames per second: 7.0\n</code></pre> <p>Based on the timing results shown, I can analyze the performance of each component of your sound spectrum analyzer: Audio Capture: 6.97 ms</p> <p>Based on the timing\u00a0results shown, I can analyze the performance of each component of\u00a0your sound spectrum analyzer:</p>"},{"location":"lessons/08-adding-timers/#1-audio-capture-697-ms","title":"1.  Audio Capture: 6.97 ms","text":"<ul> <li>This is quite efficient, taking only about 7ms\u00a0to capture the audio samples</li> <li>This is good\u00a0as it's a relatively small portion of the total processing time</li> </ul>"},{"location":"lessons/08-adding-timers/#1-fft-processing-10165-ms","title":"1.  FFT Processing: 101.65 ms","text":"<ul> <li>This is the most time-consuming operation, taking about 102ms</li> <li>This is expected as FFT is computationally intensive</li> <li>It accounts for about 71% of the total\u00a0processing time</li> </ul>"},{"location":"lessons/08-adding-timers/#1-drawing-spectrum-3358-ms","title":"1.  Drawing Spectrum: 33.58 ms","text":"<ul> <li>This is the second most time-consuming operation</li> <li>Takes about 34ms to draw\u00a0the spectrum on the OLED display</li> <li> <p>Accounts for about 24% of\u00a0the total processing time</p> </li> <li> <p>Total Processing: 142.20 ms</p> </li> <li> <p>The entire processing pipeline takes about 142ms per frame</p> </li> <li>This results\u00a0in approximately 7.0 frames per second\u00a0(1000ms/142.20ms \u2248\u00a07.0)</li> </ul>"},{"location":"lessons/08-adding-timers/#analysis-and-potential-optimizations","title":"Analysis and Potential Optimizations:","text":"<ol> <li> <p>The FFT is\u00a0the main bottleneck. You could potentially optimize this by:</p> </li> <li> <p>Further reducing the FFT size (though this would reduce frequency resolution)</p> </li> <li>Using a more optimized\u00a0FFT implementation</li> <li> <p>Pre-computing more values\u00a0if memory allows</p> </li> <li> <p>The drawing operation\u00a0is also significant. You could potentially optimize by:</p> </li> <li> <p>Reducing the number of pixels being drawn</p> </li> <li>Using a simpler visualization method</li> <li> <p>Only updating parts of the display that change</p> </li> <li> <p>The capture time is good and probably doesn't need optimization.</p> </li> </ol> <p>Would you like me to suggest\u00a0specific code optimizations for any\u00a0of these components?</p>"},{"location":"lessons/08-adding-timers/#full-code","title":"Full Code","text":"<pre><code>import time\nimport array\nimport math\nimport struct\nfrom machine import I2S, Pin, SPI\nimport ssd1306\n\n# ... existing code ...\n\ntry:\n    print(\"Optimized Sound Spectrum Analyzer\")\n    print(\"Press Ctrl+C to stop\")\n\n    # Calculate and print frequency range information\n    nyquist_freq = SAMPLE_RATE / 2\n    bin_freq_width = nyquist_freq / (FFT_SIZE // 2)\n    max_freq = int(bin_freq_width * (FFT_SIZE // 2))\n\n    print(\"Frequency resolution: {:.2f} Hz per bin\".format(bin_freq_width))\n    print(\"Display frequency range: 0 Hz to {} Hz\".format(max_freq))\n    print(\"FFT size: {}\".format(FFT_SIZE))\n    print(\"Sample rate: {} Hz\".format(SAMPLE_RATE))\n\n    # Initialize timing counters\n    capture_times = array.array('f', [0] * 100)\n    fft_times = array.array('f', [0] * 100)\n    draw_times = array.array('f', [0] * 100)\n    counter = 0\n\n    # Allocate arrays once for reuse\n    real_buffer = array.array('f', [0] * FFT_SIZE)\n    imag_buffer = array.array('f', [0] * FFT_SIZE)\n\n    # Main processing loop\n    while True:\n        # Capture audio samples with timing\n        capture_start = time.ticks_us()\n        samples = capture_audio_samples()\n        capture_end = time.ticks_us()\n\n        if samples:\n            # Get real and imaginary arrays\n            fft_real, fft_imag = samples\n\n            # Compute FFT in-place with timing\n            fft_start = time.ticks_us()\n            iterative_fft(fft_real, fft_imag)\n            fft_end = time.ticks_us()\n\n            # Calculate magnitude spectrum\n            magnitudes = calculate_magnitudes(fft_real, fft_imag)\n\n            # Draw the spectrum with timing\n            draw_start = time.ticks_us()\n            draw_spectrum(magnitudes)\n            draw_end = time.ticks_us()\n\n            # Record times\n            capture_times[counter] = time.ticks_diff(capture_end, capture_start) / 1000  # Convert to ms\n            fft_times[counter] = time.ticks_diff(fft_end, fft_start) / 1000  # Convert to ms\n            draw_times[counter] = time.ticks_diff(draw_end, draw_start) / 1000  # Convert to ms\n\n            # Calculate and display averages every 100 iterations\n            counter += 1\n            if counter &gt;= 100:\n                # Calculate averages\n                avg_capture = sum(capture_times) / 100\n                avg_fft = sum(fft_times) / 100\n                avg_draw = sum(draw_times) / 100\n                total_time = avg_capture + avg_fft + avg_draw\n\n                print(\"\\nPerformance Metrics (averaged over 100 iterations):\")\n                print(f\"Capture time: {avg_capture:.2f} ms\")\n                print(f\"FFT time: {avg_fft:.2f} ms\")\n                print(f\"Draw time: {avg_draw:.2f} ms\")\n                print(f\"Total processing time: {total_time:.2f} ms\")\n                print(f\"Frames per second: {1000/total_time:.1f}\")\n\n                # Reset counter\n                counter = 0\n\nexcept KeyboardInterrupt:\n    print(\"Monitoring stopped\")\nfinally:\n    # Clean up\n    audio_in.deinit()\n    print(\"Program terminated\")\n</code></pre>"},{"location":"lessons/09-assembler-function/","title":"Using an Assembler Version of the FFT","text":"<p>In many signal processing applications, transforming time series data into the frequency domain is a CPU intensive operation. For many systems, having a very fast FFT in assembler is critical.</p> <p>In October 2015, Peter Hinch posted a version of the FFT algorithm on his GitHub Repo written in ARM assembly.  We can use this function to increase the speed of our FFT by a factor of 7x or more.  This <code>dft.py</code> program should be uploaded to the <code>/lib</code> directory and then call it like this:</p> <pre><code># Run the FFT with the assembler function\ndft.fft(ctrl, FORWARD)\n</code></pre> <p>From Peter's README.me:</p> <pre><code>Raspberry Pi Pico 2 uses the dual-core ARM Cortex-M33 clocked at 150MHz CPU. ARM Thumb V7 instruction set with an FPU (Floating Point Unit).  The ARM FPU is so fast that integer code offers no speed advantage.\n</code></pre>"},{"location":"lessons/09-assembler-function/#pico-2-rp2350-signal-processing-hardware","title":"Pico 2 RP2350 Signal Processing Hardware","text":"<p>The ARM Cortex-M33 on the Pico 2 is very different from the original Pico.  It has faster single precision floating point and specialized DSP extensions specifically added to increase the performance of signal processing.  Specifically the Pico 2 has:</p> <ul> <li>CPU ARM Cortex-M33, dual-core \u2014 ideal for real-time + background task separation</li> <li>Instruction Set: ARMv8-M + Thumb-2</li> <li>FPU: Single-precision floating point unit (FPv5-SP)</li> <li>DSP Extensions: Yes (for signal processing tasks)</li> </ul> <p>Dual-core with independent execution could allow the FFT to run continiously on a second core while the first core gathers sound data and displays the results.</p>"},{"location":"lessons/09-assembler-function/#full-main-code-for-calling-assembler","title":"Full Main Code for Calling Assembler","text":"<p>Source Code</p> <p>Here is sample <code>main.py</code> code for running your spectrum analyzer.</p> <pre><code># Sound Spectrum Analyzer with direct use of assembler FFT\n# Combines INMP441 I2S microphone with SSD1306 OLED display\nfrom machine import I2S, Pin, SPI\nimport ssd1306\nimport math\nimport struct\nimport time\nimport array\n\nprint(\"Importing FFT modules...\")\ntry:\n    # Try to import the direct assembler FFT function\n    import dft\n    print(\"FFT module imported successfully\")\nexcept ImportError as e:\n    print(f\"Could not import FFT module: {e}\")\n\n# OLED Display configuration\nSCL = Pin(2)  # SPI Clock\nSDA = Pin(3)  # SPI Data\nRES = Pin(4)  # Reset\nDC = Pin(5)   # Data/Command\nCS = Pin(6)   # Chip Select\n\n# Initialize SPI and OLED\nprint(\"Initializing OLED display...\")\nspi = SPI(0, sck=SCL, mosi=SDA)\noled = ssd1306.SSD1306_SPI(128, 64, spi, DC, RES, CS)\n\n# I2S Microphone configuration\nSCK_PIN = 10  # Serial Clock\nWS_PIN = 11   # Word Select\nSD_PIN = 12   # Serial Data\n\n# I2S configuration parameters\nI2S_ID = 0\nSAMPLE_SIZE_IN_BITS = 32\nFORMAT = I2S.MONO\nSAMPLE_RATE = 8000\nBUFFER_LENGTH_IN_BYTES = 40000\n\n# Initialize I2S for microphone\nprint(\"Initializing I2S microphone...\")\naudio_in = I2S(\n    I2S_ID,\n    sck=Pin(SCK_PIN),\n    ws=Pin(WS_PIN),\n    sd=Pin(SD_PIN),\n    mode=I2S.RX,\n    bits=SAMPLE_SIZE_IN_BITS,\n    format=FORMAT,\n    rate=SAMPLE_RATE,\n    ibuf=BUFFER_LENGTH_IN_BYTES,\n)\n\n# FFT size (must be a power of 2)\nFFT_SIZE = 256\n\n# Create a Hanning window function\ndef hanning(x, length):\n    return 0.5 - 0.5 * math.cos(2 * math.pi * x / (length - 1))\n\n# Check if we have the direct FFT assembler function\nhave_asm_fft = hasattr(dft, 'fft')\n\n# Create the control structure for the FFT assembler function\nif have_asm_fft:\n    print(\"Setting up direct assembler FFT...\")\n    try:\n        # Create arrays for real and imaginary data\n        fft_re = array.array('f', [0] * FFT_SIZE)\n        fft_im = array.array('f', [0] * FFT_SIZE)\n\n        # Calculate bits for FFT size\n        bits = int(math.log2(FFT_SIZE))\n\n        # Create control array\n        # ctrl[0] = length of data array\n        # ctrl[1] = no. of bits to represent the length\n        # ctrl[2] = Address of real data array\n        # ctrl[3] = Address of imaginary data array\n        # ctrl[4] = Byte Offset into entry 0 of complex roots of unity\n        # ctrl[5] = Address of scratchpad for use by fft code\n        from uctypes import addressof\n        ctrl = array.array('i', [0] * 6)\n        ctrl[0] = FFT_SIZE\n        ctrl[1] = bits\n        ctrl[2] = addressof(fft_re)\n        ctrl[3] = addressof(fft_im)\n\n        # Create complex scratchpad and roots of unity\n        COMPLEX_NOS = 7\n        ROOTSOFFSET = COMPLEX_NOS * 2\n        cmplx = array.array('f', [0.0] * ((bits + 1 + COMPLEX_NOS) * 2))\n        ctrl[4] = COMPLEX_NOS * 8  # Byte offset\n        ctrl[5] = addressof(cmplx)\n\n        # Initialize u value\n        cmplx[0] = 1.0\n        cmplx[1] = 0.0\n\n        # Add scaling factor\n        cmplx[12] = 1.0 / FFT_SIZE\n        cmplx[13] = 0.0\n\n        # Calculate roots of unity\n        i = ROOTSOFFSET\n        creal = -1\n        cimag = 0\n        cmplx[i] = creal\n        cmplx[i + 1] = cimag\n        i += 2\n\n        for x in range(bits):\n            cimag = math.sqrt((1.0 - creal) / 2.0)\n            creal = math.sqrt((1.0 + creal) / 2.0)\n            cmplx[i] = creal\n            cmplx[i + 1] = cimag\n            i += 2\n\n        print(\"Direct assembler FFT setup successful\")\n        using_asm_fft = True\n    except Exception as e:\n        print(f\"Error setting up direct assembler FFT: {e}\")\n        have_asm_fft = False\n        using_asm_fft = False\n\n# Define FFT constants\nFORWARD = 1  # Forward transform\nREVERSE = 0  # Inverse transform\n\n# Convert to polar coordinates\ndef to_polar(real, imag):\n    \"\"\"Convert complex values to polar (magnitude, phase)\"\"\"\n    n = len(real)\n    for i in range(n):\n        magnitude = math.sqrt(real[i]*real[i] + imag[i]*imag[i])\n        phase = math.atan2(imag[i], real[i])\n        real[i] = magnitude\n        imag[i] = phase\n\n# Raw audio buffer for incoming samples\nraw_samples = bytearray(FFT_SIZE * 4)  # 32-bit samples = 4 bytes each\n\n# Performance monitoring variables\ntotal_capture_time = 0\ntotal_fft_time = 0\ntotal_display_time = 0\ncycle_count = 0\n\ndef capture_audio_samples():\n    \"\"\"Capture audio samples and populate the FFT input arrays\"\"\"\n    global total_capture_time\n\n    try:\n        start_time = time.ticks_us()\n\n        # Read samples from I2S microphone\n        num_bytes_read = audio_in.readinto(raw_samples)\n\n        if num_bytes_read == 0:\n            return False\n\n        # Process raw samples\n        samples_count = num_bytes_read // 4\n        format_str = \"&lt;{}i\".format(samples_count)\n        samples = struct.unpack(format_str, raw_samples[:num_bytes_read])\n\n        # Clear arrays\n        for i in range(FFT_SIZE):\n            fft_im[i] = 0.0\n\n        # Populate the real array, apply windowing\n        sample_count = min(len(samples), FFT_SIZE)\n        for i in range(sample_count):\n            # Need to shift right by 8 bits for INMP441 (24-bit samples in 32-bit words)\n            # Apply Hanning window\n            fft_re[i] = (samples[i] &gt;&gt; 8) * hanning(i, FFT_SIZE)\n\n        # Zero-pad if we didn't fill the array\n        for i in range(sample_count, FFT_SIZE):\n            fft_re[i] = 0.0\n\n        # Update timing statistics\n        elapsed = time.ticks_diff(time.ticks_us(), start_time)\n        total_capture_time += elapsed\n\n        return True\n    except Exception as e:\n        print(f\"Error in capture_audio_samples: {e}\")\n        return False\n\ndef process_fft():\n    \"\"\"Process the FFT with appropriate implementation\"\"\"\n    global total_fft_time\n\n    try:\n        start_time = time.ticks_us()\n\n        # Run the FFT with the assembler function\n        dft.fft(ctrl, FORWARD)\n\n        # Convert to polar manually\n        to_polar(fft_re, fft_im)\n\n        # Update timing statistics\n        elapsed = time.ticks_diff(time.ticks_us(), start_time)\n        total_fft_time += elapsed\n\n        return True\n    except Exception as e:\n        print(f\"Error in process_fft: {e}\")\n        return False\n\ndef draw_spectrum():\n    \"\"\"Draw the frequency spectrum on the OLED display\"\"\"\n    global total_display_time\n\n    try:\n        start_time = time.ticks_us()\n\n        # Clear the display\n        oled.fill(0)\n\n        # Number of frequency bins to display\n        num_bins = 64\n\n        # Calculate the frequency range being displayed\n        nyquist_freq = SAMPLE_RATE / 2\n        bin_freq_width = nyquist_freq / (FFT_SIZE // 2)\n        freq_end = int(bin_freq_width * (FFT_SIZE // 2))\n\n        # Combine frequency bins to fit display\n        display_bins = array.array('f', [0] * num_bins)\n        bin_width = (FFT_SIZE // 2) // num_bins\n\n        # Simple averaging of bins, focusing on lower frequencies\n        for i in range(num_bins):\n            start_idx = i * bin_width\n            end_idx = min((i + 1) * bin_width, FFT_SIZE // 2)\n\n            # Fast averaging\n            bin_sum = 0\n            for j in range(start_idx, end_idx):\n                bin_sum += fft_re[j]  # Magnitudes are in the real array after polar conversion\n            display_bins[i] = bin_sum / (end_idx - start_idx) if end_idx &gt; start_idx else 0\n\n        # Find maximum value for scaling (avoid division by zero)\n        max_magnitude = 1.0\n        for mag in display_bins:\n            if mag &gt; max_magnitude:\n                max_magnitude = mag\n\n        # Reserve top row for frequency labels\n        top_margin = 8\n\n        # Use display height minus top margin\n        display_height = 54  # 64 - top margin - 2\n        baseline = 63        # Start from the bottom of the screen\n        scaling_factor = 1.0  # Adjust scaling to distribute bars\n\n        # Find the frequency bin with the highest magnitude\n        max_idx = 0\n        max_val = 0\n        for i in range(FFT_SIZE // 2):\n            if fft_re[i] &gt; max_val:\n                max_val = fft_re[i]\n                max_idx = i\n\n        # Calculate the peak frequency in Hz\n        peak_freq = int(max_idx * bin_freq_width)\n\n        # Display peak frequency on the left and max range on the right\n        oled.text(f\"{peak_freq}\", 0, 0, 1)\n        end_text = f\"{freq_end}\"\n        # Position end frequency text right-aligned\n        end_x = 128 - (len(end_text) * 8)  # Each character is ~8 pixels wide\n        oled.text(end_text, end_x, 0, 1)\n\n        # Draw the spectrum - each bin takes 2 pixels width\n        for i in range(len(display_bins)):\n            # Apply sqrt scaling for more balanced distribution\n            normalized = display_bins[i] / max_magnitude\n            # Use sqrt scaling for better visual distribution\n            height = int(math.sqrt(normalized) * display_height * scaling_factor)\n\n            # Draw vertical bar\n            x = i * 2  # Each bar is 2 pixels wide\n            for y in range(baseline, baseline - height, -1):\n                if y &gt;= top_margin:  # Ensure we don't write over the frequency text\n                    oled.pixel(x, y, 1)\n                    oled.pixel(x + 1, y, 1)  # Make bars 2 pixels wide\n\n        # Draw baseline\n        oled.hline(0, baseline, 128, 1)\n\n        # Update the display\n        oled.show()\n\n        # Update timing statistics\n        elapsed = time.ticks_diff(time.ticks_us(), start_time)\n        total_display_time += elapsed\n\n        return True\n    except Exception as e:\n        print(f\"Error in draw_spectrum: {e}\")\n        # Try to show error on display\n        try:\n            oled.fill(0)\n            oled.text(\"FFT Error\", 0, 0, 1)\n            oled.text(str(e)[:16], 0, 10, 1)\n            oled.show()\n        except:\n            pass\n        return False\n\ntry:\n    print(\"\\n\" + \"=\"*40)\n    print(\"Sound Spectrum Analyzer\")\n    print(\"=\"*40)\n\n    # Welcome message on OLED\n    oled.fill(0)\n    oled.text(\"FFT Analyzer\", 0, 0, 1)\n    oled.text(\"Using ASM FFT\", 0, 10, 1)\n    oled.text(\"Starting...\", 0, 20, 1)\n    oled.show()\n    time.sleep(1)\n\n    # Calculate and print frequency range information\n    nyquist_freq = SAMPLE_RATE / 2\n    bin_freq_width = nyquist_freq / (FFT_SIZE // 2)\n    max_freq = int(bin_freq_width * (FFT_SIZE // 2))\n\n    print(\"\\nAnalyzer configuration:\")\n    print(f\"- Using assembler FFT: {using_asm_fft}\")\n    print(f\"- Frequency resolution: {bin_freq_width:.2f} Hz per bin\")\n    print(f\"- Display frequency range: 0 Hz to {max_freq} Hz\")\n    print(f\"- FFT size: {FFT_SIZE}\")\n    print(f\"- Sample rate: {SAMPLE_RATE} Hz\")\n    print(\"\\nStarting main loop...\")\n    print(\"Performance statistics will be displayed every 100 cycles\")\n\n    # Main processing loop\n    while True:\n        # Capture audio samples\n        if capture_audio_samples():\n            # Process FFT\n            if process_fft():\n                # Draw the spectrum\n                draw_spectrum()\n\n                # Performance monitoring\n                cycle_count += 1\n                if cycle_count &gt;= 100:\n                    # Calculate average times\n                    avg_capture = total_capture_time / cycle_count / 1000  # Convert to ms\n                    avg_fft = total_fft_time / cycle_count / 1000  # Convert to ms\n                    avg_display = total_display_time / cycle_count / 1000  # Convert to ms\n                    total_time = avg_capture + avg_fft + avg_display\n\n                    # Print performance statistics\n                    print(\"\\nPerformance Statistics (average over 100 cycles):\")\n                    print(f\"- Audio capture time: {avg_capture:.2f} ms ({(avg_capture/total_time*100):.1f}%)\")\n                    print(f\"- FFT processing time: {avg_fft:.2f} ms ({(avg_fft/total_time*100):.1f}%)\")\n                    print(f\"- Display update time: {avg_display:.2f} ms ({(avg_display/total_time*100):.1f}%)\")\n                    print(f\"- Total cycle time: {total_time:.2f} ms\")\n                    print(f\"- Theoretical max FPS: {1000/total_time:.1f}\")\n\n                    # Reset counters\n                    total_capture_time = 0\n                    total_fft_time = 0\n                    total_display_time = 0\n                    cycle_count = 0\n\nexcept KeyboardInterrupt:\n    print(\"Monitoring stopped by user\")\nexcept Exception as e:\n    print(f\"Fatal error: {e}\")\nfinally:\n    # Clean up\n    try:\n        audio_in.deinit()\n    except:\n        pass\n    print(\"Program terminated\")\n</code></pre>"},{"location":"lessons/10-future-improvements/","title":"Future Improvements","text":"<p>Prompt</p> <p>Look at the instructions in the program DFT.py in the Project Knowledge area.  Are we using all the features of the RP2350 SoC instruction set ARMv8-M + Thumb-2 and the Single-precision floating point unit (FPv5-SP)?</p>"},{"location":"lessons/10-future-improvements/#claude-response","title":"Claude Response","text":"<p>I examined the dft.py file, which contains ARM assembler code for FFT implementation. The code is not taking full advantage of the RP2350's ARMv8-M architecture and FPv5-SP floating-point capabilities.</p> <p>Key observations:</p> <ul> <li>The code is explicitly targeting ARMv7-M with Thumb instructions, as stated in the README.md: \"Requires: ARM platform with FPU supporting Arm Thumb V7 assembler.\"</li> <li>The code uses basic FPU operations like vldr, vstr, vadd, vsub, vmul, vdiv, etc., but doesn't utilize ARMv8-M specific enhancements or newer FPv5-SP features such as:</li> <li>SIMD instructions (single instruction, multiple data)</li> <li>Advanced vector operations available in newer ARM architectures</li> <li>Hardware loop instructions available in ARMv8-M</li> <li>Specialized DSP instructions</li> </ul> <p>The author even acknowledges this in the README section \"Future development\": \"Modern compilers mean that the traditional performance benefit of assembler is nonexistent unless the assembler code is hand crafted and optimised to an extreme level. This is because the compiler exploits details of the internal design of the CPU in ways which are difficult for the programmer to achieve.\"</p> <p>The code was designed for compatibility across multiple platforms (Pyboard 1.x, Pyboard D, Pico 2), which likely led to the use of a more conservative instruction set that would work across all target devices. The code is functional but could potentially be optimized further to take advantage of the RP2350's more advanced instruction set, which might result in better performance for FFT operations.</p> <p>Prompt</p> <p>What changes to improve performance would you recommend we make in DFT.py to make it faster on the ARMv7-M with Thumb instruction with the Single-precision floating point unit (FPv5-SP)?</p> <p>Based on my analysis of the <code>dft.py</code> code, here are specific optimizations to improve FFT performance on the RP2350 with ARMv7-M Thumb and FPv5-SP:</p> <ol> <li> <p>Use SIMD operations for vector processing:</p> </li> <li> <p>Replace sequential floating-point operations with SIMD instructions where possible</p> </li> <li> <p>For example, use vector load/store instructions to process multiple elements at once</p> </li> <li> <p>Optimize memory access patterns:</p> </li> <li> <p>Rearrange data structures to improve cache locality</p> </li> <li>Use pre-fetching instructions to reduce cache misses</li> <li>Align data to cache line boundaries</li> <li> <p>Use <code>pld</code> (preload) instructions before accessing memory</p> </li> <li> <p>Loop optimizations:</p> </li> <li> <p>Unroll critical loops (especially in the <code>DOMATHS</code> and FFT butterfly sections)</p> </li> <li> <p>Use fewer branches in tight loops to reduce branch prediction failures</p> </li> <li> <p>Leverage FPv5-SP specific instructions:</p> </li> <li> <p>Use fused multiply-add operations (<code>vfma</code>, <code>vfms</code>) instead of separate multiply and add</p> </li> <li> <p>These instructions can complete in a single cycle on FPv5-SP</p> </li> <li> <p>Register utilization improvements:</p> </li> <li> <p>Keep more intermediate values in FPU registers to reduce load/store operations</p> </li> <li>The current implementation frequently moves values between memory and registers</li> <li> <p>Rework register allocation to minimize register spilling</p> </li> <li> <p>Algorithm-specific optimizations:</p> </li> <li> <p>Implement radix-4 or split-radix FFT instead of radix-2</p> </li> <li>Radix-4 requires 25% fewer complex multiplications than radix-2</li> <li> <p>Consider implementing a mixed-radix approach for non-power-of-2 lengths</p> </li> <li> <p>Reduce function call overhead:</p> </li> <li> <p>Inline small assembler functions like <code>CADD</code>, <code>CSUB</code>, and <code>CMUL</code> at their call sites</p> </li> <li> <p>This eliminates the overhead of pushing/popping registers and branch instructions</p> </li> <li> <p>Specialized scaling approach:</p> </li> <li> <p>Defer scaling operations until the end of the transform</p> </li> <li> <p>Apply scaling only where necessary, not to every element</p> </li> <li> <p>Dual-issue optimization:</p> </li> <li> <p>Rearrange instructions to improve dual-issue performance on the Cortex-M7</p> </li> <li> <p>Interleave FPU and integer operations where possible</p> </li> <li> <p>Reduce twiddle factor computation overhead:</p> </li> <li> <p>Precompute more twiddle factors</p> </li> <li>Consider storing them in a more efficient format</li> </ol> <p>These optimizations could significantly improve the FFT performance, potentially getting closer to the theoretical limits of the hardware. The most substantial gains would likely come from SIMD operations, better memory access patterns, and the use of fused multiply-add instructions.</p>"},{"location":"lessons/10-future-improvements/#lowest-risk-performance-speedups","title":"Lowest Risk Performance Speedups","text":"<p>Prompt</p> <p>Of all these 10 possible changes, which ones would have the lowest risk of breaking the existing code that we could try.  We would want to do timings on both versions to test to see if the changes improved the performance.</p> <p>Looking at the possible optimizations, I'd recommend these lower-risk changes that could improve performance while minimizing the chance of breaking existing functionality:</p>"},{"location":"lessons/10-future-improvements/#1-use-fused-multiply-add-operations-replacing-separate-multiply-and-add-operations-with-vfma-and-vfms-instructions-is-relatively-straightforward-and-low-risk-for-example-in-the-cmul-function-you-could-replace","title":"1.  Use fused multiply-add operations - Replacing separate multiply and add operations with <code>vfma</code> and <code>vfms</code> instructions is relatively straightforward and low-risk. For example, in the <code>CMUL</code> function, you could replace:","text":"<pre><code>vmul(s10, s12, s14)     # s10 = ax\nvmul(s9, s13, s15)      # s9  = by\nvsub(s10, s10, s9)      # s10 = ax - by\n</code></pre> <p>with something like:</p> <pre><code>vmul(s10, s12, s14)     # s10 = ax\nvneg(s9, s13)           # s9 = -ay (if needed)\nvfms(s10, s13, s15)     # s10 = ax - (by)\n</code></pre>"},{"location":"lessons/10-future-improvements/#2-loop-unrolling","title":"2.  Loop unrolling","text":"<p>Identify the most performance-critical loops (like the butterfly operations) and manually unroll them. This is a localized change that's unlikely to break functionality while reducing branch overhead.</p>"},{"location":"lessons/10-future-improvements/#3-register-utilization-improvements","title":"3.  Register utilization improvements","text":"<p>Reducing the number of loads and stores by keeping values in registers longer is relatively safe. Look for patterns where values are stored and then immediately reloaded.</p>"},{"location":"lessons/10-future-improvements/#4-minimize-function-call-overhead","title":"4.  Minimize function call overhead","text":"<p>The CMUL, CADD, and CSUB functions are called frequently during the transform. Inlining these at their call sites would eliminate function call overhead without changing their functionality.</p>"},{"location":"lessons/10-future-improvements/#5-memory-alignment-improvements","title":"5.  Memory alignment improvements","text":"<p>Ensuring data is properly aligned for the FPU can improve performance without changing the algorithm. This might involve adjusting how arrays are allocated.</p> <p>To properly test these changes:</p> <ol> <li>Create a copy of the original code as a baseline</li> <li>Implement each optimization separately</li> <li>Run identical benchmarks on both versions</li> <li>Measure both execution time and verify results match the original implementation</li> </ol> <p>The benchmark should perform multiple FFT operations on the same data set to get a statistically significant measurement. If you have access to an oscilloscope, measuring the execution time with GPIO pins can provide more accurate timing than software methods.</p>"},{"location":"lessons/10-future-improvements/#synthetic-data-for-testing-performance","title":"Synthetic Data For Testing Performance","text":"<p>Prompt</p> <p>Could we create synthetic data to test the new version of the FFT?</p>"},{"location":"lessons/10-future-improvements/#response","title":"Response","text":"<p>Yes, creating synthetic data is an excellent approach for testing and validating FFT implementations. It provides predictable inputs with known expected outputs, making it ideal for verifying both correctness and performance improvements.</p> <p>Here's how you could create a robust synthetic test setup:</p>"},{"location":"lessons/10-future-improvements/#generating-single-frequency-sine-waves","title":"Generating Single Frequency Sine Waves","text":"<p>Simple sine waves: Generate single-frequency sine waves where you know exactly what frequency bin should show a peak:</p> <pre><code>import array\nimport math\n\ndef generate_sine(length, frequency, sample_rate=1.0):\n    \"\"\"Generate a sine wave of specified frequency\"\"\"\n    data = array.array('f', [0] * length)\n    for i in range(length):\n        t = i / sample_rate\n        data[i] = math.sin(2 * math.pi * frequency * t)\n    return data\n</code></pre>"},{"location":"lessons/10-future-improvements/#generating-multiple-frequencies","title":"Generating Multiple Frequencies","text":"<p>Multiple frequency components: Create signals with multiple known frequencies to test frequency resolution:</p> <pre><code>def generate_composite(length, frequencies, amplitudes, sample_rate=1.0):\n    \"\"\"Generate a signal with multiple frequency components\"\"\"\n    data = array.array('f', [0] * length)\n    for i in range(length):\n        t = i / sample_rate\n        for freq, amp in zip(frequencies, amplitudes):\n            data[i] += amp * math.sin(2 * math.pi * freq * t)\n    return data\n</code></pre>"},{"location":"lessons/10-future-improvements/#3-generate-special-test-cases","title":"3. Generate Special Test Cases","text":"<ul> <li>Impulse response (single value of 1.0, rest zeros)</li> <li>Square waves (rich in odd harmonics)</li> <li>Chirp signals (frequency sweep)</li> <li>Random noise (to test overall behavior)</li> </ul>"},{"location":"lessons/10-future-improvements/#4-create-a-fft-test-harness","title":"4.  Create a FFT Test Harness","text":"<pre><code>def test_fft_implementation(fft_function, test_data, expected_peaks=None):\n    \"\"\"Test an FFT implementation against known data\"\"\"\n    # Copy the data (so original isn't modified)\n    data_copy = array.array('f', test_data)\n\n    # Run the FFT\n    start_time = time.ticks_us()\n    result = fft_function(data_copy)\n    elapsed = time.ticks_diff(time.ticks_us(), start_time)\n\n    # Verify peaks if provided\n    if expected_peaks:\n        for bin_idx, expected_magnitude in expected_peaks:\n            actual = math.sqrt(result[bin_idx*2]**2 + result[bin_idx*2+1]**2)\n            assert abs(actual - expected_magnitude) &lt; 0.01, f\"Expected {expected_magnitude} at bin {bin_idx}, got {actual}\"\n\n    return elapsed\n</code></pre>"},{"location":"lessons/10-future-improvements/#5-benchmark-comparison","title":"5. Benchmark Comparison","text":"<pre><code>def compare_implementations(original_fft, modified_fft, test_cases):\n    \"\"\"Compare performance of two FFT implementations\"\"\"\n    for name, data, expected in test_cases:\n        orig_time = test_fft_implementation(original_fft, data, expected)\n        mod_time = test_fft_implementation(modified_fft, data, expected)\n        improvement = (orig_time - mod_time) / orig_time * 100\n        print(f\"{name}: Original: {orig_time}\u00b5s, Modified: {mod_time}\u00b5s, Improvement: {improvement:.2f}%\")\n</code></pre>"},{"location":"lessons/10-future-improvements/#summary","title":"Summary","text":"<p>This approach lets you:</p> <ol> <li>Verify correctness with known frequency components</li> <li>Measure performance improvements accurately</li> <li>Test edge cases and special conditions</li> <li>Ensure that optimizations don't introduce numerical inaccuracies</li> </ol> <p>For the Pico 2 specifically, incorporate different FFT sizes (128, 256, 512, 1024, 2048) to see if your optimizations have different impacts based on data size.</p>"},{"location":"lessons/50-dsp-instructions/","title":"New DSP Instructions in the Pico 2","text":"<p>Prompt</p> <p>What are the new DSP assembly language functions in the Raspberry Pi Pico 2 RP2350 instruction set ARMv8-M + Thumb-2. How can they be used to speed up a FFT algorithm</p> <p>Note</p> <p>As of March 22nd, 2025 when this was written, OpenAI's ChatGPT o1 knowledge cutoff date is June 2024. The Pico 2 was introduced on Aug 8, 2024.  So OpenAi is not aware of the Pico 2 announcement and its features.  This is why I used Perplexity since it is able to search the current internet as sources for its results.</p>"},{"location":"lessons/50-dsp-instructions/#perplexity-response","title":"Perplexity Response","text":"<p>The new Raspberry Pi Pico 2\u2019s RP2350 microcontroller introduces enhanced DSP capabilities through its ARMv8-M architecture and Thumb-2 instruction set, enabling significant performance improvements for FFT algorithms. Here\u2019s how these features are leveraged.</p>"},{"location":"lessons/50-dsp-instructions/#key-dsp-assembly-language-functions","title":"Key DSP Assembly Language Functions","text":""},{"location":"lessons/50-dsp-instructions/#1-floating-point-unit-fpu-integration","title":"1. Floating-Point Unit (FPU) Integration","text":"<p>The RP2350 includes a dedicated FPU, allowing hardware-accelerated floating-point operations critical for FFTs. This replaces slower software-based emulation used in the earlier RP2040, reducing computation time for operations like complex multiplications and additions[2][3].</p>"},{"location":"lessons/50-dsp-instructions/#2-thumb-2-instruction-set","title":"2. Thumb-2 Instruction Set","text":"<p>Thumb-2 combines 16- and 32-bit instructions, improving code density and execution efficiency. For example:</p> <pre><code>; Example: Multiply-Accumulate (MAC) operation using FPU\nVMLA.F32 S0, S1, S2  ; S0 = S0 + (S1 * S2)\n</code></pre> <p>This single-cycle MAC instruction accelerates FFT\u2019s butterfly operations[1][4].</p>"},{"location":"lessons/50-dsp-instructions/#3-dsp-extensions","title":"3. DSP Extensions","text":"<p>ARMv8-M adds DSP-oriented instructions like SIMD (Single Instruction Multiple Data) for parallel processing. For instance:</p> <p><pre><code>; SIMD example for parallel data processing\nVADD.F32 Q0, Q1, Q2  ; Add four 32-bit floats in Q1 and Q2, store in Q0\n</code></pre> This reduces loop overhead in FFT stages by processing multiple data points simultaneously[2][4].</p>"},{"location":"lessons/50-dsp-instructions/#optimizing-fft-with-rp2350-assembly","title":"Optimizing FFT with RP2350 Assembly","text":"<ol> <li>Hardware-Accelerated Arithmetic </li> <li>Replace software-based floating-point operations with FPU instructions.  </li> <li> <p>Use <code>VFP</code> registers (S0-S31) for intermediate FFT results to avoid memory bottlenecks[3][5].</p> </li> <li> <p>Loop Unrolling and Register Management    Unroll FFT butterfly loops in assembly to minimize branch penalties. Prioritize registers R0-R7 (Thumb-2\u2019s low registers) for frequent operations[1][6].    Example:</p> </li> </ol> <pre><code>; Unrolled loop for FFT stage\n.thumb_func\nfft_stage:\n    VLD1.F32 {S0-S3}, [R0]!  ; Load 4 floats\n    VADD.F32 S4, S0, S1      ; Compute real part\n    VSUB.F32 S5, S0, S1      ; Compute imaginary part\n    VST1.F32 {S4-S5}, [R1]!  ; Store results\n</code></pre> <ol> <li>Memory Optimization    Use DMA channels to asynchronously transfer audio data from I2S peripherals (e.g., microphones) to RAM, freeing the CPU for FFT computations[3][5].</li> </ol>"},{"location":"lessons/50-dsp-instructions/#performance-impact","title":"Performance Impact","text":"<p>A 256-point FFT on the RP2350 completes in 17 ms using optimized ARM assembly, compared to ~50 ms on the RP2040[3]. Key metrics:</p> <ul> <li>FPU Utilization: Reduces floating-point operation latency by 4\u201310\u00d7[2].  </li> <li>SIMD Parallelism: Processes 2\u20134 data points per instruction, cutting FFT stage time by up to 50%[4][5].  </li> </ul> <p>For real-time audio applications (e.g., spectrum analyzers), this allows:</p> <ul> <li>8 kHz sampling with 256-point FFTs at 13.1 FPS[3].  </li> <li>Lower latency for effects like reverb or noise suppression[5].</li> </ul>"},{"location":"lessons/50-dsp-instructions/#implementation-steps","title":"Implementation Steps","text":"<ol> <li>Toolchain Setup</li> </ol> <p>Use the <code>pico-sdk</code> with <code>.thumb_func</code> directives to ensure Thumb-2 compatibility[1].</p> <ol> <li>Leverage Existing Libraries </li> </ol> <p>Adapt ARM-optimized FFT libraries (e.g., Peter Hinch\u2019s FFT library) to exploit RP2350-specific instructions[3][5].</p> <ol> <li>Profiling</li> </ol> <p>Use cycle-counting tools to identify bottlenecks in FFT stages and replace them with inline assembly[6].</p> <p>By integrating these techniques, developers can achieve real-time FFT performance on the Pico 2 for audio processing, spectral analysis, and IoT applications.</p> <p>Citations: [1] https://blog.smittytone.net/2022/06/19/get-started-with-arm-assembly-on-the-pi-pico/ [2] https://www.reddit.com/r/synthdiy/comments/1enb1m2/raspberry_pi_pico_2_announced_with_floating_point/ [3] https://www.reddit.com/r/raspberrypipico/comments/1jg1j9r/fft_sound_spectrum_analyzer_running_on_a/ [4] https://dsp.stackexchange.com/questions/96118/raspberry-pi-picorp2040-or-rp2350-asm-pio-micropython-for-fft-dsp [5] https://people.ece.cornell.edu/land/courses/ece4760/RP2350/FFT_iFFT/index_FFT_iFFT.html [6] https://forums.raspberrypi.com/viewtopic.php?t=301003 [7] https://forums.raspberrypi.com/viewtopic.php?t=381317 [8] https://www.reddit.com/r/embedded/comments/1en9752/raspberry_pi_pico_2/ [9] https://spotpear.com/wiki/Raspberry-Pi-Pico-2-RP2350.html [10] https://www.youtube.com/watch?v=mszrdmg-LGs [11] https://www.youtube.com/watch?v=_O3Aeyv2ILM [12] https://www.all-about-industries.com/raspberry-pi-pico-2-improvements-and-details-a-333778caf5af2721e0d27535c0acd8f4/ [13] https://codalogic.com/blog/2023/01/07/Pico-Assembly-Programming [14] http://www.ulisp.com/show?4X21 [15] https://www.raspberrypi.com/documentation/microcontrollers/pico-series.html [16] https://www.mathworks.com/help/dsp/ref/dsp.fft-system-object.html [17] https://www.youtube.com/watch?v=z6Al3_AuHgQ [18] https://www.raspberrypi.com/news/accelerating-fourier-transforms-using-the-gpu/</p> <p>Answer from Perplexity: pplx.ai/share</p>"},{"location":"setup/","title":"Setting Up Your Spectrum Analyzer Environment","text":"<p>Getting your environment setup to create your own personal Spectrum Analyzer or a Spectrum Analyzer for your own STEM classroom is your first step.  This section will guide you through all the steps from purchasing the best low-cost parts to creating a fun desktop environment.</p>"},{"location":"setup/#purchasing-your-hardware","title":"Purchasing Your Hardware","text":"<p>Hardware</p>"},{"location":"setup/#configuring-your-desktop","title":"Configuring your Desktop","text":"<p>Desktop</p>"},{"location":"setup/#connecting-your-components","title":"Connecting Your Components","text":"<p>Connections</p>"},{"location":"setup/connections/","title":"Breadboard Connections","text":""},{"location":"setup/connections/#display-harness","title":"Display Harness","text":"<p>Display Cable Harness</p>"},{"location":"setup/desktop/","title":"Desktop Setup","text":""},{"location":"setup/desktop/#thonny","title":"Thonny","text":"<p>We strongly recommend that newcomers to this project start out with the student-friendly Thonny environment.</p> <p>There is an excellent \"Getting Started with Thonny\" guide on our Moving Rainbow site.  This guide includes detailed step</p>"},{"location":"setup/hardware/","title":"Spectrum Analyzer Hardware","text":""},{"location":"setup/hardware/#raspberry-pi-pico-2","title":"Raspberry Pi Pico 2","text":"<p>The original Pico will work, however the clock is slower and it does not have built-in floating point hardware.</p>"},{"location":"setup/hardware/#microphone","title":"Microphone","text":"<p>We like the INMP442 since it has an I2S interface and very low noise.</p>"},{"location":"setup/hardware/#display","title":"Display","text":"<p>We use a 2.42\" OLED display with a SSD1306 driver.</p> <p>If you're prototyping on a breadboard or using standard jumper wires, stick with 4\u20138 MHz to ensure signal integrity and reduce data corruption.</p>"},{"location":"setup/hardware/#breadboard","title":"Breadboard","text":"<p>Breadboards</p>"},{"location":"sims/","title":"MicroSims for the Spectrum Analysis","text":""},{"location":"sims/#tone-generator","title":"Tone Generator","text":"<p>Tone Generator</p>"},{"location":"sims/#base-and-treble","title":"Base and Treble","text":"<p>Base and Treble</p>"},{"location":"sims/#waveform-types","title":"Waveform Types","text":"<p>Waveform Types</p>"},{"location":"sims/base-and-treble/","title":"Base and Treble Amplifier Controls","text":"<p>Run the Base and Treble MicroSim Edit the Base and Treble MicroSim</p> <pre><code>I would like to create a p5.js simulation of the frequency\nresponse of an old classic amplifier with base and treble knobs.\nInstead of knobs, use two sliders at the bottom of the canvas,\none for base and one for treble.\nThe sliders should start out in the middle.\n\nUse the following p5.js template for placement of the slider controls.\n</code></pre> <pre><code>// region drawing parameters\n// the width of the entire canvas\nlet canvasWidth = 400;\n// The top drawing region above the interactive controls\nlet drawHeight = 400;\n// control region height\nlet controlHeight = 50;\n// The total hight of both the drawing region height + the control region height\nlet canvasHeight = 450;\n// margin around the active plotting region\nlet margin = 25;\n// larger text so students in the back of the room can read the labels\nlet defaultTextSize = 16;\n\nfunction setup {\n    const canvas = createCanvas(canvasWidth, canvasHeight);\n    // canvas.parent('canvas-container');\n    var mainElement = document.querySelector('main');\n    canvas.parent(mainElement);\n    textSize(defaultTextSize);\n\n    // create a new slider at th bottom of the canvas\n    mySlider = createSlider(0, 100, 50, 1);\n    mySlider.position(sliderLeftMargin, drawHeight + 12);\n    mySlider.style('width', canvasWidth -  sliderLeftMargin - 20 + 'px');\n\n}\n\nfunction draw() {\n    // make the background drawing region a very light blue\n    fill('aliceblue');\n    rect(0, 0, canvasWidth, drawHeight);\n    // make the background of the controls area white\n    fill('white')\n    rect(0, drawHeight, canvasWidth, controlHeight);\n\n    // get the updated slider value\n    val = mySlider.value();\n\n    // put your custom code here\n\n\n    // draw label and value\n    text(\"MySlider: \" +  val, 10, drawHeight + 25)\n}\n</code></pre>"},{"location":"sims/base-and-treble/#response","title":"Response","text":"<p>ChatGPT o1-preview Thought for 52 seconds</p> <p>Certainly! Below is the modified p5.js code that simulates the frequency response of a classic amplifier with bass and treble controls using two sliders. The sliders start in the middle and adjust the bass and treble settings. The frequency response is plotted on the canvas, showing how the settings affect the amplifier's output.</p> <pre><code>// region drawing parameters\n// the width of the entire canvas\nlet canvasWidth = 400;\n// The top drawing region above the interactive controls\nlet drawHeight = 400;\n// control region height\nlet controlHeight = 80;\n// The total height of both the drawing region height + the control region height\nlet canvasHeight = drawHeight + controlHeight;\n// margin around the active plotting region\nlet margin = 25;\n// larger text so students in the back of the room can read the labels\nlet defaultTextSize = 16;\n\nlet bassSlider, trebleSlider;\n\nfunction setup() {\n  const canvas = createCanvas(canvasWidth, canvasHeight);\n  var mainElement = document.querySelector('main');\n  canvas.parent(mainElement);\n  textSize(defaultTextSize);\n\n  // Define slider left margin\n  let sliderLeftMargin = margin;\n\n  // Create bass slider\n  bassSlider = createSlider(0, 100, 50, 1);\n  bassSlider.position(sliderLeftMargin, drawHeight + 12);\n  bassSlider.style('width', canvasWidth - sliderLeftMargin - 20 + 'px');\n\n  // Create treble slider\n  trebleSlider = createSlider(0, 100, 50, 1);\n  trebleSlider.position(sliderLeftMargin, drawHeight + 12 + 30);\n  trebleSlider.style('width', canvasWidth - sliderLeftMargin - 20 + 'px');\n}\n\nfunction draw() {\n  // Set background color\n  background('white');\n\n  // Fill the drawing region with 'aliceblue'\n  fill('aliceblue');\n  noStroke();\n  rect(0, 0, canvasWidth, drawHeight);\n\n  // Fill the control region with 'white'\n  fill('white');\n  rect(0, drawHeight, canvasWidth, controlHeight);\n\n  // Get the updated slider values\n  let bassVal = bassSlider.value();\n  let trebleVal = trebleSlider.value();\n\n  // Normalize the slider values to range -1 to 1\n  let bassAmount = (bassVal - 50) / 50;\n  let trebleAmount = (trebleVal - 50) / 50;\n\n  // Define frequency range and number of points\n  let numPoints = 500;\n  let freqMin = 20;\n  let freqMax = 20000;\n\n  // Define EQ parameters\n  let f_bass_cutoff = 500;\n  let f_treble_cutoff = 2000;\n  let n = 4;\n  let maxGain = 12; // dB\n  let gainMin = -15; // dB\n  let gainMax = 15; // dB\n\n  // Begin shape for plotting the frequency response curve\n  noFill();\n  stroke(0);\n  beginShape();\n\n  for (let i = 0; i &lt; numPoints; i++) {\n    let fraction = i / (numPoints - 1);\n    let freq = freqMin * Math.pow(freqMax / freqMin, fraction);\n\n    // Compute lowShelf and highShelf\n    let lowShelf = 1 / (1 + Math.pow(freq / f_bass_cutoff, n));\n    let highShelf = 1 / (1 + Math.pow(f_treble_cutoff / freq, n));\n\n    // Compute gains\n    let bassGain_dB = bassAmount * maxGain * lowShelf;\n    let trebleGain_dB = trebleAmount * maxGain * highShelf;\n\n    let totalGain_dB = bassGain_dB + trebleGain_dB;\n\n    // Map frequency to x position\n    let x = map(\n      Math.log10(freq),\n      Math.log10(freqMin),\n      Math.log10(freqMax),\n      margin,\n      canvasWidth - margin\n    );\n\n    // Map gain to y position\n    let y = map(totalGain_dB, gainMin, gainMax, drawHeight - margin, margin);\n\n    // Plot the point\n    vertex(x, y);\n  }\n\n  endShape();\n\n  // Draw axes\n  stroke(150);\n  // x-axis at y = gain 0 dB\n  let yZero = map(0, gainMin, gainMax, drawHeight - margin, margin);\n  line(margin, yZero, canvasWidth - margin, yZero);\n\n  // Draw labels for frequencies\n  fill(0);\n  textAlign(CENTER, TOP);\n  text(\"Frequency Response\", canvasWidth / 2, margin / 2);\n\n  // Draw labels for sliders\n  textAlign(LEFT, CENTER);\n  fill(0);\n  text(\"Bass\", bassSlider.x + 5, bassSlider.y - 10);\n  text(\"Treble\", trebleSlider.x + 5, trebleSlider.y - 10);\n}\n`\n</code></pre> <p>Explanation:</p> <ul> <li> <p>Sliders Creation: Two sliders, <code>bassSlider</code> and <code>trebleSlider</code>, are created in the <code>setup()</code> function. They are positioned one below the other in the control area at the bottom of the canvas.</p> </li> <li> <p>Slider Values Normalization: The values from the sliders are normalized to a range of -1 to 1 to represent the adjustment from minimum to maximum gain.</p> </li> <li> <p>Frequency Response Calculation: The frequency response is calculated using shelving filter approximations for bass and treble adjustments. The gains are computed based on the slider positions and applied to low and high-frequency ranges.</p> </li> <li> <p>Plotting the Curve: The frequency response curve is plotted using <code>beginShape()</code> and <code>vertex()</code>. Frequencies are mapped logarithmically to the x-axis, and gains in decibels are mapped to the y-axis.</p> </li> <li> <p>Axes and Labels: Axes are drawn to represent 0 dB gain, and labels are added for clarity. The labels for the sliders are positioned above them.</p> </li> </ul> <p>Note: This code should be run in a p5.js environment. The canvas size and control regions are set according to the template provided. Adjustments can be made to the EQ parameters to simulate different amplifier characteristics.</p>"},{"location":"sims/base-and-treble/#refinement-of-placement-of-sliders","title":"Refinement of Placement of Sliders","text":"<pre><code>That was perfect. Now, please place the base slider\non the left and the treble slider on the right.\nPlace the labels under the sliders.\n\n## ChatGPT Response\n\nChatGPT o1-preview Thought for 31 seconds\n\nCertainly! Below is the updated p5.js code with the bass slider placed on the left and the treble slider on the right, with labels positioned under the sliders.\n\n```javascript\n// region drawing parameters\n// the width of the entire canvas\nlet canvasWidth = 400;\n// The top drawing region above the interactive controls\nlet drawHeight = 400;\n// control region height\nlet controlHeight = 80;\n// The total height of both the drawing region height + the control region height\nlet canvasHeight = drawHeight + controlHeight;\n// margin around the active plotting region\nlet margin = 25;\n// larger text so students in the back of the room can read the labels\nlet defaultTextSize = 16;\n\nlet bassSlider, trebleSlider;\n\nfunction setup() {\n  const canvas = createCanvas(canvasWidth, canvasHeight);\n  var mainElement = document.querySelector('main');\n  canvas.parent(mainElement);\n  textSize(defaultTextSize);\n\n  // Define slider left margin\n  let sliderLeftMargin = margin;\n\n  // Define slider width (half the canvas width minus margins)\n  let sliderWidth = (canvasWidth - 3 * margin) / 2;\n\n  // Create bass slider on the left\n  bassSlider = createSlider(0, 100, 50, 1);\n  bassSlider.position(sliderLeftMargin, drawHeight + 12);\n  bassSlider.style('width', sliderWidth + 'px');\n\n  // Create treble slider on the right\n  trebleSlider = createSlider(0, 100, 50, 1);\n  trebleSlider.position(sliderLeftMargin + sliderWidth + margin, drawHeight + 12);\n  trebleSlider.style('width', sliderWidth + 'px');\n}\n\nfunction draw() {\n  // Set background color\n  background('white');\n\n  // Fill the drawing region with 'aliceblue'\n  fill('aliceblue');\n  noStroke();\n  rect(0, 0, canvasWidth, drawHeight);\n\n  // Fill the control region with 'white'\n  fill('white');\n  rect(0, drawHeight, canvasWidth, controlHeight);\n\n  // Get the updated slider values\n  let bassVal = bassSlider.value();\n  let trebleVal = trebleSlider.value();\n\n  // Normalize the slider values to range -1 to 1\n  let bassAmount = (bassVal - 50) / 50;\n  let trebleAmount = (trebleVal - 50) / 50;\n\n  // Define frequency range and number of points\n  let numPoints = 500;\n  let freqMin = 20;\n  let freqMax = 20000;\n\n  // Define EQ parameters\n  let f_bass_cutoff = 500;\n  let f_treble_cutoff = 2000;\n  let n = 4;\n  let maxGain = 12; // dB\n  let gainMin = -15; // dB\n  let gainMax = 15; // dB\n\n  // Begin shape for plotting the frequency response curve\n  noFill();\n  stroke(0);\n  beginShape();\n\n  for (let i = 0; i &lt; numPoints; i++) {\n    let fraction = i / (numPoints - 1);\n    let freq = freqMin * Math.pow(freqMax / freqMin, fraction);\n\n    // Compute lowShelf and highShelf\n    let lowShelf = 1 / (1 + Math.pow(freq / f_bass_cutoff, n));\n    let highShelf = 1 / (1 + Math.pow(f_treble_cutoff / freq, n));\n\n    // Compute gains\n    let bassGain_dB = bassAmount * maxGain * lowShelf;\n    let trebleGain_dB = trebleAmount * maxGain * highShelf;\n\n    let totalGain_dB = bassGain_dB + trebleGain_dB;\n\n    // Map frequency to x position\n    let x = map(\n      Math.log10(freq),\n      Math.log10(freqMin),\n      Math.log10(freqMax),\n      margin,\n      canvasWidth - margin\n    );\n\n    // Map gain to y position\n    let y = map(totalGain_dB, gainMin, gainMax, drawHeight - margin, margin);\n\n    // Plot the point\n    vertex(x, y);\n  }\n\n  endShape();\n\n  // Draw axes\n  stroke(150);\n  // x-axis at y = gain 0 dB\n  let yZero = map(0, gainMin, gainMax, drawHeight - margin, margin);\n  line(margin, yZero, canvasWidth - margin, yZero);\n\n  // Draw labels\n  fill(0);\n  textAlign(CENTER, TOP);\n  text(\"Frequency Response\", canvasWidth / 2, margin / 2);\n\n  // Draw labels for sliders under the sliders\n  textAlign(CENTER, TOP);\n  fill(0);\n  text(\"Bass\", bassSlider.x + bassSlider.width / 2, bassSlider.y + 20);\n  text(\"Treble\", trebleSlider.x + trebleSlider.width / 2, trebleSlider.y + 20);\n}\n</code></pre> <p>Explanation:</p> <ul> <li> <p>Slider Placement: The bass slider is placed on the left side of the control area, and the treble slider is placed on the right side. Both sliders have equal widths, calculated to fit within the canvas with appropriate margins.</p> </li> <li> <p>Slider Labels: The labels for \"Bass\" and \"Treble\" are positioned under the sliders by adjusting the <code>text()</code> function parameters in the <code>draw()</code> function.</p> </li> <li> <p>Slider Width Calculation:</p> </li> </ul> <pre><code> let sliderWidth = (canvasWidth - 3 * margin) / 2;\n ```\n\nThis calculation ensures that both sliders fit side by side within the canvas width, accounting for margins.\n\n-   **Slider Positioning:**\n\n-   **Bass Slider:**\n\n```javascript\nbassSlider.position(sliderLeftMargin, drawHeight + 12);\n</code></pre> <ul> <li>Treble Slider:</li> </ul> <pre><code> trebleSlider.position(sliderLeftMargin + sliderWidth + margin, drawHeight + 12);\n</code></pre> <ul> <li>Label Positioning: The labels are centered under each slider by using:</li> </ul> <pre><code>textAlign(CENTER, TOP);\n    text(\"Bass\", bassSlider.x + bassSlider.width / 2, bassSlider.y + 20);\n</code></pre> <ul> <li>Overall Layout: The control area at the bottom now neatly contains both sliders side by side with labels underneath, improving the user interface and meeting your requirements.</li> </ul> <p>Note: This code should work in a p5.js environment. Feel free to adjust the margins or slider widths if necessary to fit your specific layout preferences.</p>"},{"location":"sims/templates/","title":"MicroSim Template","text":"<p>Run the MicroSim Edit this MicroSim</p>"},{"location":"sims/templates/#sample-iframe","title":"Sample iframe","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/microsims/sims/name/main.html\" height=\"400\"  scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/templates/#about-this-microsim","title":"About this MicroSim","text":"<p>This MicroSim teaches us how to specify the drawing of a static filled circle.</p>"},{"location":"sims/templates/#sample-prompt","title":"Sample Prompt","text":"<p>Prompt</p> <p>Create a single file p5.js sketch.   Draw a green circle on a 600x400 canvas with a radius of 200.</p>"},{"location":"sims/templates/#sample-response","title":"Sample Response","text":"<pre><code>// canvas regions setup\n// let canvasWidth = 750;\nlet drawHeight = 400;\nlet controlHeight = 50;\nlet canvasHeight = drawHeight + controlHeight;\nlet aspectRatio = 1.91; // Open Graph standard\nlet canvasWidth = canvasHeight * aspectRatio;\nlet radius = 200;\n\nfunction setup() {\n  const canvas = createCanvas(canvasWidth, canvasHeight);\n  var mainElement = document.querySelector('main');\n  canvas.parent(mainElement);\n}\n\nfunction draw() {\n  // background of drawing region\n  fill('aliceblue');\n  rect(0, 0, canvasWidth, drawHeight);\n\n  // background of controls\n  fill('whitegray');\n  rect(0, drawHeight, canvasWidth, controlHeight);\n  circle(canvasWidth/2, canvasHeight/2, radius);\n}\n</code></pre>"},{"location":"sims/templates/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/templates/#grade-level-subject-and-topic","title":"Grade Level, Subject and Topic","text":"<p>9th grade geometry. Circle and radius.</p>"},{"location":"sims/templates/#activities","title":"Activities","text":""},{"location":"sims/templates/#fill-color","title":"Fill Color","text":"<p>Change the color in the prompt.  You can specify any one of the 140 named colors.</p>"},{"location":"sims/templates/#border-weight","title":"Border Weight","text":"<p>Change the prompt to include a black border of width 10.</p>"},{"location":"sims/templates/#border-color","title":"Border Color","text":"<p>Change the prompt to make the border purple.</p>"},{"location":"sims/templates/#change-the-radius","title":"Change the Radius","text":"<p>Change the prompt to make the circle smaller or larger by changing the radius from 10 to 200.</p>"},{"location":"sims/templates/#change-the-location","title":"Change the Location","text":"<p>Change the prompt to move the circle to the left or right.</p>"},{"location":"sims/templates/#references","title":"References","text":"<ul> <li>Processing Wiki on Positioning Your Canvas</li> </ul>"},{"location":"sims/tone-generator/","title":"Tone Generator MicroSim","text":"<p>Run the Tone Generator MicroSim Edit this MicroSim</p>"},{"location":"sims/tone-generator/#sample-iframe","title":"Sample iframe","text":"<p>Just copy the iframe below and paste it into your web page.</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/spectrum-analyzer/sims/tone-generator/main.html\" height=\"250\"  scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>This MicroSim was created using the Cursor IDE loaded with a rules file that generates p5.js MicroSims.</p> <p>For more on MicroSims see the MicroSims intelligent textbook.</p>"},{"location":"sims/waveform-types/","title":"Waveform Types MicroSim","text":"<p>Run the Waveform Types MicroSim Edit the Waveform Types MicroSim</p>"},{"location":"sims/waveform-types/#sample-iframe","title":"Sample iframe","text":"<p>Just copy the iframe below and paste it into your web page.</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/spectrum-analyzer/sims/waveform-types/main.html\" height=\"350\"  scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre> <p>This MicroSim was created using the Cursor IDE loaded with a rules file that generates p5.js MicroSims.</p> <p>For more on MicroSims see the MicroSims intelligent textbook.</p>"}]}