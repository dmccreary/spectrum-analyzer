{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spectrum Analyzer","text":"<p>Welcome to the website on using the Raspberry Pi Pico 2 as a spectrum analyzer.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#term","title":"Term","text":"<p>This is the definition of the term.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"research/","title":"Research","text":"<p>Prompt</p> <p>I have a Raspberry Pi Pico 2 that has a dual core RP2350 system on a chip.  Each core is a Cortex-M33 with floating point and DSP support.</p> <p>I want to run sound signal processing on these devices and run algorithms like the FFT that leverage the new floating point and the new DSP support.</p> <p>Can you fine any documentation and sample FFT code that has been finely tuned for the Pico 2?</p> <p>I am already aware of Peter Hinch's implementation of FFT in ARM assembly from his GitHub repo.  However, his code is over 10 years old and clearly does not use any of the new features on the Pico 2.</p> <p>Note that Peter's FFT in ARM assembly is here: https://github.com/peterhinch/micropython-fourier/blob/master/dft.py Note that it has not changed since 2015.</p> <p>I am looking for authoritative information and working code that will take advantage of every feature of the Pico 2 including using dual-cores to read in sound and write FFT spectrums to a display.  I would prefer code that also has a test suite and benchmarking tools on synthetic data.</p> <p>Great, I\u2019ll look for documentation and optimized FFT code for the Raspberry Pi Pico 2 (RP2350) that is compatible with MicroPython and designed to leverage its dual-core Cortex-M33 architecture, FPU, and DSP support. I\u2019ll also search for examples that include performance benchmarks or testing tools, ideally with MicroPython calling into assembly or C. I\u2019ll get back to you shortly with what I find.</p>"},{"location":"research/#raspberry-pi-pico-2-fft-documentation-optimized-implementations","title":"Raspberry Pi Pico 2 FFT: Documentation &amp; Optimized Implementations","text":""},{"location":"research/#hardware-dsp-capabilities-of-rp2350-pico-2","title":"Hardware DSP Capabilities of RP2350 (Pico 2)","text":"<p>The Raspberry Pi Pico 2 is built on the RP2350 microcontroller, which features dual Arm Cortex-M33 cores running up to 150\u00a0MHz. In contrast to the original Pico\u2019s M0+ cores, the M33 cores include a single-precision FPU (Floating Point Unit) and Armv8-M DSP instructions (Introducing the RP2350 | Stephen Smith's Blog). This means the Pico 2 can perform floating-point math in hardware and use SIMD-like DSP operations (e.g. single-cycle multiply-accumulate) for faster signal processing. Official datasheets and ARM\u2019s documentation note these enhancements, which significantly improve algorithms like FFT on this platform (Introducing the RP2350 | Stephen Smith's Blog). (The RP2350 also has a dual-core RISC-V cluster, but only one of the two architectures can run at a time.) </p>"},{"location":"research/#authoritative-references","title":"Authoritative references","text":"<ul> <li>RP2350 Datasheet / Tech Docs: The Raspberry Pi Foundation provides documentation for the RP2350, detailing its Cortex-M33 cores, FPU support, and DSP instruction set. These instructions accelerate common DSP operations (fixed-point arithmetic, saturating Q15/Q31 math, etc.), benefiting FFT and filtering routines.  </li> <li>ARM Cortex-M33 Docs: ARM\u2019s official manuals describe the M33\u2019s instruction set and extensions. Notably, the M33\u2019s DSP extension adds optimized arithmetic instructions, and the single-precision FPU executes IEEE754 float operations in hardware (Introducing the RP2350 | Stephen Smith's Blog). Together, these enable much faster FFT computation than on the original Pico (which lacked an FPU or DSP extensions).</li> </ul>"},{"location":"research/#optimized-fft-libraries-for-cortex-m33-rp2350","title":"Optimized FFT Libraries for Cortex-M33 (RP2350)","text":"<p>ARM CMSIS-DSP Library: The CMSIS-DSP library is an ARM-developed collection of highly optimized DSP functions for Cortex-M cores (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). It includes FFT implementations for various data types (32-bit float, 64-bit float, Q15, Q31 fixed-point) that automatically leverage the hardware FPU and DSP instructions when available (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). For example, CMSIS-DSP provides functions like <code>arm_rfft_fast_f32</code> (for real FFT on floats) and <code>arm_cfft_q15</code> (complex FFT on Q15 fixed-point) among many others (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). On the Pico 2\u2019s M33, these routines run very efficiently \u2013 using the FPU for floating-point FFTs or SIMD fixed-point arithmetic for Q15/Q31 FFTs. ARM\u2019s documentation explains that the \u201cFast\u201d real-FFT algorithms exploit symmetry to halve the computation versus naive complex FFTs (CMSIS-DSP: Real FFT Functions) (CMSIS-DSP: Real FFT Functions). </p> <p>Key resources: - CMSIS-DSP Documentation \u2013 The user manual (on ARM\u2019s GitHub pages) details each FFT function and usage (CMSIS-DSP: Real FFT Functions). It covers initialization (e.g. <code>arm_rfft_fast_init_f32</code>), input/output data formats, and optimized algorithms for real data (CMSIS-DSP: Real FFT Functions) (CMSIS-DSP: Real FFT Functions). This is an authoritative reference for how to use these functions and what hardware features they exploit. - CMSIS-DSP Example Code \u2013 There are many examples of using CMSIS-DSP FFT on microcontrollers (for instance, STM32 examples and application notes (STM32 Fast Fourier Transform (CMSIS DSP FFT) - Phil's Lab #111)). These can be adapted to Pico 2. A community project by jptrainor benchmarked CMSIS-DSP on the original Pico, demonstrating how to integrate the library (in C/C++) and measure performance (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). (On an M0+ Pico, float FFTs had to rely on software, but on the M33 Pico 2 the same code will use the hardware FPU for a big speedup.)  </p> <p>KISS FFT (Pico FFT Library): Another option is the lightweight KISS FFT (\u201cKeep It Simple FFT\u201d) library, which is written in portable C. A community member created a Pico-oriented wrapper called pico_fft based on KISS FFT (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.). It simplifies capturing ADC data and computing an FFT on the Pico. While KISS FFT isn\u2019t as optimized as CMSIS-DSP, it\u2019s easier to understand and has a small footprint. The pico_fft project includes documentation and examples of connecting an analog microphone to the Pico and performing an FFT on the samples (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.). All examples (in C using the Pico SDK) have been tested, and they even provide a quick-start guide for hardware setup and code building (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.) (GitHub - Googool/pico_fft: A lightweight and efficient FFT (Fast Fourier Transform) library for the Raspberry Pi Pico, based on the KISS FFT library.). This could be useful if you prefer a self-contained library. Keep in mind that on the RP2350, a KISS FFT (written in C) will still benefit from the M33\u2019s speed and can use the FPU for float math (via the compiler) \u2013 but it won\u2019t explicitly use SIMD DSP instructions unless manually optimized. </p> <p>Performance notes: On the Pico 2, a well-optimized FFT is quite fast. For instance, Peter Hinch\u2019s assembly FFT (discussed below) can compute a 1024-point real FFT in about 7\u00a0ms on a Pico 2 (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). In comparison, the same on a classic Pico M0+ might take tens of milliseconds or more since it lacks the FPU (python - Raspberry Pi Pico(RP2040 or RP2350) ASM PIO microPython for FFT, DSP - Signal Processing Stack Exchange) (python - Raspberry Pi Pico(RP2040 or RP2350) ASM PIO microPython for FFT, DSP - Signal Processing Stack Exchange). CMSIS-DSP\u2019s routines should achieve similar performance to the assembly approach, as they are built to leverage the same hardware features. The CMSIS library also supports smaller FFT sizes efficiently (e.g. 256-point, 512-point), often using mixed-radix algorithms for speed. ARM\u2019s benchmarks (in CMSIS docs) show orders of magnitude speedup vs. naive FFT implementations thanks to these optimizations (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). </p>"},{"location":"research/#dual-core-audio-processing-examples","title":"Dual-Core Audio Processing Examples","text":"<p>One advantage of the RP2350 (Pico 2) is its dual-core CPU, which you can use to parallelize tasks \u2013 ideal for real-time audio capture and FFT display. A great example is Iwatake\u2019s dual-core spectrum analyzer project. It runs on the original Pico (RP2040) but demonstrates the concept well: Core0 handles data acquisition from a microphone via ADC (using DMA) and driving an SPI display, while Core1 performs the FFT calculations on the incoming data stream (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io). This way, the heavy math on Core1 doesn\u2019t stall sampling or screen updates on Core0. The project was written in C++ using the official Pico SDK, and Iwatake has released the full source code on GitHub (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io) (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io). (The code uses ADC + DMA to fill buffers, triggers an IRQ or flag when a buffer is ready, then Core1 processes the buffer with an FFT and sends the result to Core0 for display.) This design achieved a functional real-time spectrum display, although the author noted a occasional bug causing freezes (likely a synchronization issue) (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io). It\u2019s a useful reference for structuring dual-core tasks and using the Pico\u2019s multicore FIFO or interrupts to communicate between cores.</p> <p>Another community example is a project by Van Hunter Adams at Cornell: Realtime Audio FFT to VGA on RP2040. While it ran on a single-core (RP2040) with heavy use of DMA and PIO, it illustrates high-speed ADC capture (10 kHz) with DMA, and performing a 1024-point fixed-point FFT on the fly (FFT) (FFT). On the Pico 2, one could adapt this idea and split the work across cores for even better throughput (e.g., Core0 managing continuous DMA ADC sampling, Core1 doing the FFT and sending results to a VGA or LCD driver). The code and write-up for that project are available on Adams\u2019s site (FFT) (FFT) and could serve as a starting point for handling real-time data streams.</p> <p>For a more hobbyist-friendly approach, there\u2019s an Arduino-based spectrum analyzer for RP2040 by Bodmer that uses one core but cleverly combines DMA and an SPI TFT update. It samples ADC at ~14 kHz, computes an FFT (64\u2013512 points), and draws the spectrum on a 320x240 TFT. The code uses the Earle Philhower Arduino core (which has built-in support for the RP2040) and likely leverages the CMSIS-DSP library under the hood (the Arduino <code>mbed_rp2040</code> core includes CMSIS-DSP) or a simple FFT routine. It achieves about 54 FPS for a 256-point FFT visualization (and up to 81 FPS for 64-point) by using DMA for sampling and fast SPI for drawing (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT) (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT). The source is available on GitHub as \u201cADC_DMA_FFT\u201d with documentation in the README (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT) (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT). While this was tested on RP2040, the same code on an RP2350 could be tweaked to use both cores or simply enjoy the extra performance headroom (e.g. to increase FFT size or sample rate). It\u2019s a good, well-documented reference for implementing an end-to-end audio FFT pipeline on the Pico family.</p> <p>Key takeaways for dual-core design: Use DMA to offload data movement (e.g. ADC to memory), use one core exclusively for I/O (sensor reads, drawing to display) and the other for computation, and use thread-safe queues or interrupts to hand off data between cores. The Pico SDK provides <code>multicore_fifo_push</code>/<code>pop</code> functions and even higher-level primitives to coordinate the two cores. In FreeRTOS or other RTOS environments, you could assign tasks to cores and use mutexes/queues. The examples above show that a dual-core Pico can comfortably handle real-time audio FFT at audio-rate sampling (8\u201344 kHz) and update a display, especially when each core\u2019s workload is optimized.</p>"},{"location":"research/#micropython-and-assembly-integration","title":"MicroPython and Assembly Integration","text":"<p>MicroPython on the Pico 2 lets you write high-level code, but Python by itself is too slow for real-time FFT on microcontrollers. The good news is MicroPython supports calling assembly-optimized routines for performance-critical parts. There are a few ways to do this:</p> <ul> <li> <p>Inline ARM Thumb Assembly: MicroPython has an inline assembler for ARM Cortex-M. By decorating a function with <code>@micropython.asm_thumb</code>, you can write pure assembly instructions that MicroPython will assemble and execute natively (Adding Assembly Language to MicroPython | Stephen Smith's Blog) (Adding Assembly Language to MicroPython | Stephen Smith's Blog). You can pass a few arguments in registers (r0\u2013r3) and use any ARMv7E-M Thumb-2 instructions \u2013 including those for floating point on M33 (MicroPython\u2019s assembler includes opcodes like <code>vadd.f32</code>, etc., which will use the FPU) (Adding Assembly Language to MicroPython | Stephen Smith's Blog). This is ideal for writing a custom FFT inner loop or other DSP routines. The official docs and tutorials (e.g. Damien George\u2019s hints, or Stephen Smith\u2019s blog (Adding Assembly Language to MicroPython | Stephen Smith's Blog)) cover how to structure these functions. Keep in mind you must manage registers and follow the MicroPython calling convention, but you can mix Python and assembly seamlessly (call the asm function from Python with normal arguments). </p> </li> <li> <p>FFI / Native Modules: Another approach is to compile C or assembly code as a native module and import it in MicroPython. MicroPython\u2019s build system allows adding C extensions that become importable as Python modules. For example, one could compile the CMSIS-DSP library (or just the needed FFT function) into the firmware and expose a Python-callable function. This requires customizing the firmware build, but there are community discussions and examples on doing this (for instance, wrapping CMSIS-DSP functions in a MicroPython module) (CMSIS DSP and NN micropython wrappers \u00b7 micropython \u00b7 Discussion #10200 \u00b7 GitHub). There isn\u2019t an off-the-shelf MicroPython \u201cnumpy\u201d with DSP yet (though it\u2019s been contemplated (DSP CMSIS - MicroPython Forum (Archive)) (DSP CMSIS - MicroPython Forum (Archive))), but the ulab module is a notable alternative. ulab is a NumPy-like module for MicroPython (written in C) that provides vectorized operations and an FFT routine (<code>ulab.numpy.fft.fft</code>) in a Pythonic way (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). It\u2019s not specific to the RP2350, but it\u2019s optimized C code, so it runs much faster than Python loops and can handle moderate-sized FFTs. If using MicroPython, you might consider using ulab\u2019s FFT for simplicity \u2013 it won\u2019t explicitly use the M33 DSP instructions, but in C it will still be fast, and you avoid writing assembly by hand.</p> </li> <li> <p>Peter Hinch\u2019s Assembly FFT Library: For an authoritative, ready-made solution, Peter Hinch\u2019s <code>micropython-fourier</code> library is highly recommended. This is a MicroPython library implementing a fast FFT (actually a DFT class) almost entirely in ARM assembly, specifically tuned for boards with an FPU (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). It was written for the Pyboard (STM32F7) and has been updated to support the Pico 2\u2019s M33 core. The library computes single-precision FFTs in-place, uses precomputed twiddle factors, and does no heap allocation, meaning it can even be called in an interrupt handler for real-time use (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). It also offers conveniences like window functions and conversion to magnitude or dB. According to the author\u2019s benchmarks, a 1024-point FFT takes ~6.97\u00a0ms on the Pico 2 (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). Even a 256-point FFT would be only a fraction of that (the Reddit post below measured ~17\u00a0ms for 256-point including overhead in Python) (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). Hinch\u2019s code uses the M33\u2019s FPU for arithmetic, so it is floating-point based (more dynamic range than fixed-point). The repository includes documentation (README with usage notes) and a test script. To use it, you can copy the <code>.py</code> files to your Pico and import the <code>DFT</code> class in MicroPython. This gives you an object where you can populate data and run <code>fft()</code> to get the spectrum. (If you want to integrate with live ADC data, you\u2019d sample into a Python array or <code>array.array('f')</code>, then call the assembly routine to transform it.)</p> <p>Reference: \u201cFast Fourier transform in MicroPython\u2019s inline ARM assembler\u201d by Peter Hinch (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) \u2013 This is the library\u2019s README, which explains the design and usage. It explicitly notes it \u201crequires an ARM platform with FPU supporting Arm Thumb V7 assembler (e.g. Pyboard D, Pico\u00a02)\u201d (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). In other words, it\u2019s tailored for the Cortex-M4/M7/M33 class of devices. The README also provides a performance section comparing runtime on different boards (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.), confirming the Pico 2\u2019s advantage thanks to its 2\u00d7 clock and FPU/DSP support. </p> </li> <li> <p>MicroPython Dual-Core: It\u2019s worth noting that MicroPython on RP2040 currently runs on a single core by default. Using the second core is non-trivial in MicroPython (there is no built-in thread or task offload to the second core in the high-level API as of now). Advanced users have experimented with launching assembly code on the second core (via the <code>machine.mem</code> and writing to the second core\u2019s start address), but this is complex. If dual-core operation is a requirement, writing that portion in C (and maybe invoking it via MicroPython\u2019s FFI) or using an RTOS might be easier. However, given the Pico 2\u2019s speed, many audio FFT applications can be done on one core in MicroPython if the heavy math is in optimized assembly. For instance, user Dan McCreary demonstrated a MicroPython spectrum analyzer on the Pico 2: using an I2S microphone at 8 kHz, a 256-point FFT in assembly (~17\u00a0ms), and updating an OLED display (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico) (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). His results showed the FFT using only ~22% of the frame time (with the display update taking more time), so the optimized routine left plenty of CPU headroom (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). This suggests that offloading the math to assembly was enough to meet real-time requirements without needing the second core in MicroPython. Documentation for his project is available on his \u201cLearning MicroPython\u201d site, and it specifically credits Peter Hinch\u2019s FFT library for the speedup (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico) (FFT sound spectrum analyzer running on a Raspberry Pi Pico 2. : r/raspberrypipico). </p> </li> </ul>"},{"location":"research/#conclusion-and-further-resources","title":"Conclusion and Further Resources","text":"<p>In summary, the Raspberry Pi Pico 2\u2019s RP2350 chip brings substantial DSP capability through its dual M33 cores with FPU/DSP extensions. To exploit this for fast FFTs: </p> <ul> <li>Use optimized libraries like CMSIS-DSP for C/C++ projects \u2013 these give you proven, assembly-tuned FFT functions that harness the hardware (floating or fixed-point) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico). The official ARM docs and Raspberry Pi\u2019s examples (e.g. CMSIS integration in the SDK (New RP2040 CMSIS Pack - Raspberry Pi) or Arduino\u2019s CMSIS-DSP package) can guide you through setup. </li> <li>Study real-time examples (like Iwatake\u2019s dual-core analyzer (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io) or Bodmer\u2019s DMA FFT display (GitHub - Bodmer/ADC_DMA_FFT: RP2040: sample ADC, run FFT, display on TFT)) to understand how to pipeline data acquisition and processing. They illustrate techniques like double-buffering ADC data with DMA, using one core per task, and optimizing I/O throughput \u2013 all relevant to high-performance audio applications on the Pico 2. </li> <li>Leverage MicroPython\u2019s extensibility if you prefer Python at a high level. MicroPython by itself is not fast enough for DSP, but by injecting assembly (either inline or via a library) you can achieve performance close to a C implementation. Peter Hinch\u2019s FFT library is an excellent, well-documented starting point for MicroPython (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.). Additionally, the <code>ulab</code> module can handle many numeric tasks in C for you, including FFT, which might simplify development for prototyping.</li> </ul> <p>By combining these resources, you can capture audio in real-time, compute an FFT with minimal latency, and display the spectrum \u2013 fully utilizing the Pico 2\u2019s hardware acceleration. Be sure to refer to the official Raspberry Pi Pico C/C++ SDK documentation for details on using the ADC, DMA, PIO, and multicore features, as those will be the backbone for any high-speed real-time processing on this platform. With the right approach, the tiny $5 Pico\u00a02 can perform serious signal processing tasks that were previously possible only on much more expensive or power-hungry systems.</p> <p>Sources: Documentation and code from Raspberry Pi Foundation and ARM, and community projects and libraries on GitHub: RP2350 hardware specs (Introducing the RP2350 | Stephen Smith's Blog), ARM CMSIS-DSP library API (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico) (GitHub - jptrainor/cmsis-sandbox: Benchmarking CMSIS-DSP on Rasberry Pi Pico), dual-core FFT example (iwatake) (Iwatake Turns the Raspberry Pi Pico Into a Dual-Core, FFT-Calculating Live Spectrum Analyzer - Hackster.io), MicroPython assembly FFT by Peter Hinch (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.) (GitHub - peterhinch/micropython-fourier: Fast Fourier transform in MicroPython's inline ARM assembler.), and others as cited above.</p>"}]}